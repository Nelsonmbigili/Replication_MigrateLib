{
  "exitcode": 1,
  "summary": {
    "passed": 18,
    "failed": 7,
    "total": 25,
    "collected": 25
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": ".",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "arxiv",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "arxiv.egg-info",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/test_api_bugs.py::TestAPIBugs",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_api_bugs.py::TestAPIBugs::test_missing_title",
          "type": "TestCaseFunction",
          "lineno": 8
        }
      ]
    },
    {
      "nodeid": "tests/test_api_bugs.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_api_bugs.py::TestAPIBugs",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/test_client.py::TestClient",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_client.py::TestClient::test_invalid_format_id",
          "type": "TestCaseFunction",
          "lineno": 18
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_invalid_id",
          "type": "TestCaseFunction",
          "lineno": 22
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_max_results",
          "type": "TestCaseFunction",
          "lineno": 35
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_no_duplicates",
          "type": "TestCaseFunction",
          "lineno": 95
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_nonexistent_id_in_list",
          "type": "TestCaseFunction",
          "lineno": 26
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_offset",
          "type": "TestCaseFunction",
          "lineno": 66
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_query_page_count",
          "type": "TestCaseFunction",
          "lineno": 41
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_retry",
          "type": "TestCaseFunction",
          "lineno": 102
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_search_results_offset",
          "type": "TestCaseFunction",
          "lineno": 82
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_sleep_between_errors",
          "type": "TestCaseFunction",
          "lineno": 174
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_sleep_elapsed",
          "type": "TestCaseFunction",
          "lineno": 150
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_sleep_multiple_requests",
          "type": "TestCaseFunction",
          "lineno": 136
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_sleep_standard",
          "type": "TestCaseFunction",
          "lineno": 122
        },
        {
          "nodeid": "tests/test_client.py::TestClient::test_sleep_zero_delay",
          "type": "TestCaseFunction",
          "lineno": 165
        }
      ]
    },
    {
      "nodeid": "tests/test_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_client.py::TestClient",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/test_download.py::TestDownload",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_download.py::TestDownload::test_download_from_query",
          "type": "TestCaseFunction",
          "lineno": 21
        },
        {
          "nodeid": "tests/test_download.py::TestDownload::test_download_tarfile_from_query",
          "type": "TestCaseFunction",
          "lineno": 42
        },
        {
          "nodeid": "tests/test_download.py::TestDownload::test_download_with_custom_slugify_from_query",
          "type": "TestCaseFunction",
          "lineno": 53
        }
      ]
    },
    {
      "nodeid": "tests/test_download.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_download.py::TestDownload",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/test_package.py::TestPackage",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_package.py::TestPackage::test_deprecated_import_pattern",
          "type": "TestCaseFunction",
          "lineno": 15
        }
      ]
    },
    {
      "nodeid": "tests/test_package.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_package.py::TestPackage",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/test_result.py::TestResult",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_result.py::TestResult::test_eq",
          "type": "TestCaseFunction",
          "lineno": 71
        },
        {
          "nodeid": "tests/test_result.py::TestResult::test_from_feed_entry",
          "type": "TestCaseFunction",
          "lineno": 45
        },
        {
          "nodeid": "tests/test_result.py::TestResult::test_get_short_id",
          "type": "TestCaseFunction",
          "lineno": 51
        },
        {
          "nodeid": "tests/test_result.py::TestResult::test_legacy_ids",
          "type": "TestCaseFunction",
          "lineno": 97
        },
        {
          "nodeid": "tests/test_result.py::TestResult::test_result_shape",
          "type": "TestCaseFunction",
          "lineno": 37
        },
        {
          "nodeid": "tests/test_result.py::TestResult::test_to_datetime",
          "type": "TestCaseFunction",
          "lineno": 59
        }
      ]
    },
    {
      "nodeid": "tests/test_result.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_result.py::TestResult",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_api_bugs.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_download.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_package.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_result.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": ".",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "arxiv",
          "type": "Package"
        },
        {
          "nodeid": "arxiv.egg-info",
          "type": "Dir"
        },
        {
          "nodeid": "tests",
          "type": "Package"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/test_api_bugs.py::TestAPIBugs::test_missing_title",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_missing_title",
        "TestAPIBugs",
        "test_api_bugs.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_invalid_format_id",
      "lineno": 18,
      "outcome": "failed",
      "keywords": [
        "test_invalid_format_id",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 21,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 579,
            "message": "in _results"
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=0)\nurl = 'https://export.arxiv.org/api/query?search_query=&id_list=abc&sortBy=relevance&sortOrder=descending&start=0&max_results=100'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n        resp = future.result()  # Wait for the response\n        self._last_request_dt = datetime.now()\n        if resp.status_code != 200:\n>           raise HTTPError(url, try_index, resp.status_code)\nE           arxiv.HTTPError: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=abc&sortBy=relevance&sortOrder=descending&start=0&max_results=100)\n\narxiv\\__init__.py:664: HTTPError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_invalid_format_id>\n\n    def test_invalid_format_id(self):\n        with self.assertRaises(arxiv.HTTPError):\n>           list(arxiv.Client(num_retries=0).results(arxiv.Search(id_list=[\"abc\"])))\n\ntests\\test_client.py:21: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\narxiv\\__init__.py:579: in _results\n    feed = self._parse_feed(page_url, first_page=True)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_invalid_id",
      "lineno": 22,
      "outcome": "passed",
      "keywords": [
        "test_invalid_id",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_max_results",
      "lineno": 35,
      "outcome": "passed",
      "keywords": [
        "test_max_results",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_no_duplicates",
      "lineno": 95,
      "outcome": "passed",
      "keywords": [
        "test_no_duplicates",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_nonexistent_id_in_list",
      "lineno": 26,
      "outcome": "passed",
      "keywords": [
        "test_nonexistent_id_in_list",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_offset",
      "lineno": 66,
      "outcome": "passed",
      "keywords": [
        "test_offset",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_query_page_count",
      "lineno": 41,
      "outcome": "passed",
      "keywords": [
        "test_query_page_count",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_retry",
      "lineno": 102,
      "outcome": "failed",
      "keywords": [
        "test_retry",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 112,
            "message": ""
          },
          {
            "path": "tests\\test_client.py",
            "lineno": 110,
            "message": "in broken_get"
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 579,
            "message": "in _results"
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_retry>\nmock_sleep = <MagicMock name='sleep' id='2359308392864'>\nmock_get = <MagicMock name='get' id='2359308401696'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(500))\n    @patch(\"time.sleep\", return_value=None)\n    def test_retry(self, mock_sleep, mock_get):\n        broken_client = arxiv.Client()\n    \n        def broken_get():\n            search = arxiv.Search(query=\"quantum\")\n            return next(broken_client.results(search))\n    \n>       self.assertRaises(arxiv.HTTPError, broken_get)\n\ntests\\test_client.py:112: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_client.py:110: in broken_get\n    return next(broken_client.results(search))\narxiv\\__init__.py:579: in _results\n    feed = self._parse_feed(page_url, first_page=True)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_search_results_offset",
      "lineno": 82,
      "outcome": "passed",
      "keywords": [
        "test_search_results_offset",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_sleep_between_errors",
      "lineno": 174,
      "outcome": "failed",
      "keywords": [
        "test_sleep_between_errors",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 181,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "self = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_sleep_between_errors>\nmock_sleep = <MagicMock name='sleep' id='2359308934592'>\nmock_get = <MagicMock name='get' id='2359308928448'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(500))\n    @patch(\"time.sleep\", return_value=None)\n    def test_sleep_between_errors(self, mock_sleep, mock_get):\n        client = arxiv.Client()\n        url = client._format_url(arxiv.Search(query=\"quantum\"), 0, 1)\n        try:\n>           client._parse_feed(url)\n\ntests\\test_client.py:181: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_sleep_elapsed",
      "lineno": 150,
      "outcome": "failed",
      "keywords": [
        "test_sleep_elapsed",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 158,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "self = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_sleep_elapsed>\nmock_sleep = <MagicMock name='sleep' id='2359308934208'>\nmock_get = <MagicMock name='get' id='2359309021168'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(200))\n    @patch(\"time.sleep\", return_value=None)\n    def test_sleep_elapsed(self, mock_sleep, mock_get):\n        client = arxiv.Client()\n        url = client._format_url(arxiv.Search(query=\"quantum\"), 0, 1)\n        # If _last_request_dt is less than delay_seconds ago, sleep.\n        client._last_request_dt = datetime.now() - timedelta(seconds=client.delay_seconds - 1)\n>       client._parse_feed(url)\n\ntests\\test_client.py:158: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_sleep_multiple_requests",
      "lineno": 136,
      "outcome": "failed",
      "keywords": [
        "test_sleep_multiple_requests",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 145,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "self = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_sleep_multiple_requests>\nmock_sleep = <MagicMock name='sleep' id='2359309014352'>\nmock_get = <MagicMock name='get' id='2359304421936'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(200))\n    @patch(\"time.sleep\", return_value=None)\n    def test_sleep_multiple_requests(self, mock_sleep, mock_get):\n        client = arxiv.Client()\n        url1 = client._format_url(arxiv.Search(query=\"quantum\"), 0, 1)\n        url2 = client._format_url(arxiv.Search(query=\"testing\"), 0, 1)\n        # Rate limiting is URL-independent; expect same behavior as in\n        # `test_sleep_standard`.\n>       client._parse_feed(url1)\n\ntests\\test_client.py:145: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_sleep_standard",
      "lineno": 122,
      "outcome": "failed",
      "keywords": [
        "test_sleep_standard",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 129,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "self = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_sleep_standard>\nmock_sleep = <MagicMock name='sleep' id='2359308720640'>\nmock_get = <MagicMock name='get' id='2359308723424'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(200))\n    @patch(\"time.sleep\", return_value=None)\n    def test_sleep_standard(self, mock_sleep, mock_get):\n        client = arxiv.Client()\n        url = client._format_url(arxiv.Search(query=\"quantum\"), 0, 1)\n        # A client should sleep until delay_seconds have passed.\n>       client._parse_feed(url)\n\ntests\\test_client.py:129: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_client.py::TestClient::test_sleep_zero_delay",
      "lineno": 165,
      "outcome": "failed",
      "keywords": [
        "test_sleep_zero_delay",
        "__wrapped__",
        "patchings",
        "TestClient",
        "test_client.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\__init__.py",
          "lineno": 630,
          "message": "NameError: name 'requests' is not defined"
        },
        "traceback": [
          {
            "path": "tests\\test_client.py",
            "lineno": 171,
            "message": ""
          },
          {
            "path": "arxiv\\__init__.py",
            "lineno": 630,
            "message": "NameError"
          }
        ],
        "longrepr": "self = arxiv.Client(page_size=100, delay_seconds=0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n>           return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n\narxiv\\__init__.py:626: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, try_index = 0\n\n    def __try_parse_feed(\n        self,\n        url: str,\n        first_page: bool,\n        try_index: int,\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Recursive helper for _parse_feed. Enforces `self.delay_seconds`: if that\n        number of seconds has not passed since `_parse_feed` was last called,\n        sleeps until delay_seconds seconds have passed.\n        \"\"\"\n        # If this call would violate the rate limit, sleep until it doesn't.\n        if self._last_request_dt is not None:\n            required = timedelta(seconds=self.delay_seconds)\n            since_last_request = datetime.now() - self._last_request_dt\n            if since_last_request < required:\n                to_sleep = (required - since_last_request).total_seconds()\n                logger.info(\"Sleeping: %f seconds\", to_sleep)\n                time.sleep(to_sleep)\n    \n        logger.info(\"Requesting page (first: %r, try: %d): %s\", first_page, try_index, url)\n    \n        future = self._session.get(url, headers={\"user-agent\": \"arxiv.py/2.1.3\"})\n>       resp = future.result()  # Wait for the response\nE       AttributeError: 'Response' object has no attribute 'result'\n\narxiv\\__init__.py:661: AttributeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <tests.test_client.TestClient testMethod=test_sleep_zero_delay>\nmock_sleep = <MagicMock name='sleep' id='2359308728416'>\nmock_get = <MagicMock name='get' id='2359308533584'>\n\n    @patch(\"requests_futures.sessions.FuturesSession.get\", return_value=empty_response(200))\n    @patch(\"time.sleep\", return_value=None)\n    def test_sleep_zero_delay(self, mock_sleep, mock_get):\n        client = arxiv.Client(delay_seconds=0)\n        url = client._format_url(arxiv.Search(query=\"quantum\"), 0, 1)\n>       client._parse_feed(url)\n\ntests\\test_client.py:171: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = arxiv.Client(page_size=100, delay_seconds=0, num_retries=3)\nurl = 'https://export.arxiv.org/api/query?search_query=quantum&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=1'\nfirst_page = True, _try_index = 0\n\n    def _parse_feed(\n        self, url: str, first_page: bool = True, _try_index: int = 0\n    ) -> feedparser.FeedParserDict:\n        \"\"\"\n        Fetches the specified URL and parses it with feedparser.\n    \n        If a request fails or is unexpectedly empty, retries the request up to\n        `self.num_retries` times.\n        \"\"\"\n        try:\n            return self.__try_parse_feed(url, first_page=first_page, try_index=_try_index)\n        except (\n            HTTPError,\n            UnexpectedEmptyPageError,\n>           requests.exceptions.ConnectionError,\n        ) as err:\nE       NameError: name 'requests' is not defined\n\narxiv\\__init__.py:630: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_download.py::TestDownload::test_download_from_query",
      "lineno": 21,
      "outcome": "passed",
      "keywords": [
        "test_download_from_query",
        "TestDownload",
        "test_download.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_download.py::TestDownload::test_download_tarfile_from_query",
      "lineno": 42,
      "outcome": "passed",
      "keywords": [
        "test_download_tarfile_from_query",
        "TestDownload",
        "test_download.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_download.py::TestDownload::test_download_with_custom_slugify_from_query",
      "lineno": 53,
      "outcome": "passed",
      "keywords": [
        "test_download_with_custom_slugify_from_query",
        "TestDownload",
        "test_download.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_package.py::TestPackage::test_deprecated_import_pattern",
      "lineno": 15,
      "outcome": "passed",
      "keywords": [
        "test_deprecated_import_pattern",
        "TestPackage",
        "test_package.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_eq",
      "lineno": 71,
      "outcome": "passed",
      "keywords": [
        "test_eq",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_from_feed_entry",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_from_feed_entry",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_get_short_id",
      "lineno": 51,
      "outcome": "passed",
      "keywords": [
        "test_get_short_id",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_legacy_ids",
      "lineno": 97,
      "outcome": "passed",
      "keywords": [
        "test_legacy_ids",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_result_shape",
      "lineno": 37,
      "outcome": "passed",
      "keywords": [
        "test_result_shape",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_result.py::TestResult::test_to_datetime",
      "lineno": 59,
      "outcome": "passed",
      "keywords": [
        "test_to_datetime",
        "TestResult",
        "test_result.py",
        "tests",
        "lukasschwab@arxiv.py__a1180d0e__requests__requests_futures",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    }
  ],
  "warnings": [
    {
      "message": "**Deprecated** after 2.1.0; use 'import arxiv' instead.",
      "category": "UserWarning",
      "when": "collect",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\arxiv\\arxiv.py",
      "lineno": 10
    },
    {
      "message": "The 'Search.results' method is deprecated, use 'Client.results' instead",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\tests\\test_api_bugs.py",
      "lineno": 22
    },
    {
      "message": "The 'Search.results' method is deprecated, use 'Client.results' instead",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\tests\\test_client.py",
      "lineno": 24
    },
    {
      "message": "The 'Search.results' method is deprecated, use 'Client.results' instead",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\tests\\test_client.py",
      "lineno": 99
    },
    {
      "message": "The 'Search.results' method is deprecated, use 'Client.results' instead",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\tests\\test_download.py",
      "lineno": 11
    },
    {
      "message": "The 'Search.results' method is deprecated, use 'Client.results' instead",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\lukasschwab@arxiv.py__a1180d0e__requests__requests_futures\\tests\\test_download.py",
      "lineno": 12
    }
  ]
}