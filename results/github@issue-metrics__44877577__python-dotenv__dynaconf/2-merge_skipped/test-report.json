{
  "exitcode": 1,
  "summary": {
    "passed": 75,
    "failed": 23,
    "total": 98,
    "collected": 98
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": ".",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "docs/img",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "docs",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "docs/img",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_invalid_credentials",
          "type": "TestCaseFunction",
          "lineno": 111
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_ghe",
          "type": "TestCaseFunction",
          "lineno": 48
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_ghe_and_ghe_app",
          "type": "TestCaseFunction",
          "lineno": 58
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_github_app",
          "type": "TestCaseFunction",
          "lineno": 21
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_token",
          "type": "TestCaseFunction",
          "lineno": 32
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_without_authentication_information",
          "type": "TestCaseFunction",
          "lineno": 40
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_get_github_app_installation_token",
          "type": "TestCaseFunction",
          "lineno": 72
        },
        {
          "nodeid": "test_auth.py::TestAuthToGithub::test_get_github_app_installation_token_request_failure",
          "type": "TestCaseFunction",
          "lineno": 91
        }
      ]
    },
    {
      "nodeid": "test_auth.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_auth.py::TestAuthToGithub",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_config.py::TestGetIntFromEnv",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var",
          "type": "TestCaseFunction",
          "lineno": 32
        },
        {
          "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var_with_empty_env_var",
          "type": "TestCaseFunction",
          "lineno": 40
        },
        {
          "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var_with_non_integer",
          "type": "TestCaseFunction",
          "lineno": 50
        }
      ]
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_auth_with_github_app_installation_missing_inputs",
          "type": "TestCaseFunction",
          "lineno": 329
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_missing_query",
          "type": "TestCaseFunction",
          "lineno": 214
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_missing_token",
          "type": "TestCaseFunction",
          "lineno": 197
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_optional_values",
          "type": "TestCaseFunction",
          "lineno": 232
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_optionals_are_defaulted",
          "type": "TestCaseFunction",
          "lineno": 287
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_with_github_app",
          "type": "TestCaseFunction",
          "lineno": 91
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_with_token",
          "type": "TestCaseFunction",
          "lineno": 145
        }
      ]
    },
    {
      "nodeid": "test_config.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_config.py::TestGetIntFromEnv",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_config.py::TestGetEnvVars",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_does_not_exist_and_default_value_returns_false",
          "type": "TestCaseFunction",
          "lineno": 64
        },
        {
          "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_does_not_exist_and_default_value_returns_true",
          "type": "TestCaseFunction",
          "lineno": 50
        },
        {
          "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_false",
          "type": "TestCaseFunction",
          "lineno": 24
        },
        {
          "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_false_due_to_invalid_value",
          "type": "TestCaseFunction",
          "lineno": 36
        },
        {
          "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_true",
          "type": "TestCaseFunction",
          "lineno": 12
        }
      ]
    },
    {
      "nodeid": "test_config_get_bool.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_config_get_bool.py::TestEnv",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_discussions.py::TestGetDiscussions",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_error_status_code",
          "type": "TestCaseFunction",
          "lineno": 116
        },
        {
          "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_graphql_error",
          "type": "TestCaseFunction",
          "lineno": 128
        },
        {
          "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_multiple_pages",
          "type": "TestCaseFunction",
          "lineno": 69
        },
        {
          "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_single_page",
          "type": "TestCaseFunction",
          "lineno": 29
        }
      ]
    },
    {
      "nodeid": "test_discussions.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_discussions.py::TestGetDiscussions",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetEnvVars",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_issue_metrics.py::TestGetEnvVars::test_get_env_vars",
          "type": "TestCaseFunction",
          "lineno": 30
        },
        {
          "nodeid": "test_issue_metrics.py::TestGetEnvVars::test_get_env_vars_missing_query",
          "type": "TestCaseFunction",
          "lineno": 45
        }
      ]
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_with_hide_envs",
          "type": "TestCaseFunction",
          "lineno": 59
        },
        {
          "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_with_ignore_users",
          "type": "TestCaseFunction",
          "lineno": 277
        },
        {
          "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_without_hide_envs",
          "type": "TestCaseFunction",
          "lineno": 168
        }
      ]
    },
    {
      "nodeid": "test_issue_metrics.py::TestDiscussionMetrics",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_issue_metrics.py::TestDiscussionMetrics::test_get_per_issue_metrics_with_discussion",
          "type": "TestCaseFunction",
          "lineno": 402
        },
        {
          "nodeid": "test_issue_metrics.py::TestDiscussionMetrics::test_get_per_issue_metrics_with_discussion_with_hide_envs",
          "type": "TestCaseFunction",
          "lineno": 436
        }
      ]
    },
    {
      "nodeid": "test_issue_metrics.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_issue_metrics.py::TestGetEnvVars",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_issue_metrics.py::TestDiscussionMetrics",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_json_writer.py::TestWriteToJson",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_json_writer.py::TestWriteToJson::test_write_to_json",
          "type": "TestCaseFunction",
          "lineno": 16
        },
        {
          "nodeid": "test_json_writer.py::TestWriteToJson::test_write_to_json_with_no_response",
          "type": "TestCaseFunction",
          "lineno": 134
        }
      ]
    },
    {
      "nodeid": "test_json_writer.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_json_writer.py::TestWriteToJson",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_labels.py::TestLabels",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_labels.py::TestLabels::test_get_label_events",
          "type": "TestCaseFunction",
          "lineno": 50
        },
        {
          "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_closed_issue",
          "type": "TestCaseFunction",
          "lineno": 59
        },
        {
          "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_closed_issue_labeled_past_closed_at",
          "type": "TestCaseFunction",
          "lineno": 88
        },
        {
          "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_open_issue",
          "type": "TestCaseFunction",
          "lineno": 66
        }
      ]
    },
    {
      "nodeid": "test_labels.py::TestGetAverageTimeInLabels",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_labels.py::TestGetAverageTimeInLabels::test_get_stats_time_in_labels",
          "type": "TestCaseFunction",
          "lineno": 113
        }
      ]
    },
    {
      "nodeid": "test_labels.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_labels.py::TestLabels",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_labels.py::TestGetAverageTimeInLabels",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers::test_markdown_too_large_for_issue_body",
          "type": "TestCaseFunction",
          "lineno": 13
        },
        {
          "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers::test_split_markdown_file",
          "type": "TestCaseFunction",
          "lineno": 34
        }
      ]
    },
    {
      "nodeid": "test_markdown_helpers.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdown",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown",
          "type": "TestCaseFunction",
          "lineno": 31
        },
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown_no_issues",
          "type": "TestCaseFunction",
          "lineno": 251
        },
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown_with_vertical_bar_in_title",
          "type": "TestCaseFunction",
          "lineno": 143
        }
      ]
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdownWithEnv",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdownWithEnv::test_writes_markdown_file_with_non_hidden_columns_only",
          "type": "TestCaseFunction",
          "lineno": 299
        }
      ]
    },
    {
      "nodeid": "test_markdown_writer.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdown",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_markdown_writer.py::TestWriteToMarkdownWithEnv",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_count_comments_per_user_limit",
          "type": "TestCaseFunction",
          "lineno": 24
        },
        {
          "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_count_comments_per_user_with_ignores",
          "type": "TestCaseFunction",
          "lineno": 56
        },
        {
          "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_get_mentor_count",
          "type": "TestCaseFunction",
          "lineno": 95
        }
      ]
    },
    {
      "nodeid": "test_most_active_mentors.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_search.py::TestSearchIssues",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_just_owner_or_org",
          "type": "TestCaseFunction",
          "lineno": 49
        },
        {
          "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_just_owner_or_org_with_bypass",
          "type": "TestCaseFunction",
          "lineno": 73
        },
        {
          "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_owner_and_repository",
          "type": "TestCaseFunction",
          "lineno": 26
        }
      ]
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owner_and_repositories_without_repo_in_query",
          "type": "TestCaseFunction",
          "lineno": 122
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_multiple_entries",
          "type": "TestCaseFunction",
          "lineno": 133
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_org",
          "type": "TestCaseFunction",
          "lineno": 141
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_user",
          "type": "TestCaseFunction",
          "lineno": 147
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_without_either_in_query",
          "type": "TestCaseFunction",
          "lineno": 128
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_with_owner_and_repo_in_query",
          "type": "TestCaseFunction",
          "lineno": 116
        }
      ]
    },
    {
      "nodeid": "test_search.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_search.py::TestSearchIssues",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_search.py::TestGetOwnerAndRepository",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_multiple_intervals",
          "type": "TestCaseFunction",
          "lineno": 57
        },
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_no_draft_events",
          "type": "TestCaseFunction",
          "lineno": 101
        },
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_ongoing_draft",
          "type": "TestCaseFunction",
          "lineno": 83
        },
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_with_ready_for_review",
          "type": "TestCaseFunction",
          "lineno": 22
        },
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_without_ready_for_review",
          "type": "TestCaseFunction",
          "lineno": 40
        },
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_without_ready_for_review_and_closed",
          "type": "TestCaseFunction",
          "lineno": 111
        }
      ]
    },
    {
      "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_empty_list",
          "type": "TestCaseFunction",
          "lineno": 169
        },
        {
          "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_no_data",
          "type": "TestCaseFunction",
          "lineno": 155
        },
        {
          "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_with_data",
          "type": "TestCaseFunction",
          "lineno": 134
        }
      ]
    },
    {
      "nodeid": "test_time_in_draft.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_none_for_empty_list",
          "type": "TestCaseFunction",
          "lineno": 18
        },
        {
          "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_none_for_list_with_no_time_to_answer",
          "type": "TestCaseFunction",
          "lineno": 29
        },
        {
          "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_stats_time_to_answer",
          "type": "TestCaseFunction",
          "lineno": 46
        }
      ]
    },
    {
      "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_none_if_answer_chosen_at_is_missing",
          "type": "TestCaseFunction",
          "lineno": 81
        },
        {
          "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_none_if_created_at_is_missing",
          "type": "TestCaseFunction",
          "lineno": 95
        },
        {
          "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_time_to_answer",
          "type": "TestCaseFunction",
          "lineno": 109
        }
      ]
    },
    {
      "nodeid": "test_time_to_answer.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose::test_get_stats_time_to_close",
          "type": "TestCaseFunction",
          "lineno": 23
        },
        {
          "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose::test_get_stats_time_to_close_no_issues",
          "type": "TestCaseFunction",
          "lineno": 51
        }
      ]
    },
    {
      "nodeid": "test_time_to_close.py::TestMeasureTimeToClose",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close",
          "type": "TestCaseFunction",
          "lineno": 75
        },
        {
          "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close_discussion",
          "type": "TestCaseFunction",
          "lineno": 97
        },
        {
          "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close_returns_none",
          "type": "TestCaseFunction",
          "lineno": 88
        }
      ]
    },
    {
      "nodeid": "test_time_to_close.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_time_to_close.py::TestMeasureTimeToClose",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response",
          "type": "TestCaseFunction",
          "lineno": 27
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_bot",
          "type": "TestCaseFunction",
          "lineno": 341
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_issue_owners_comment",
          "type": "TestCaseFunction",
          "lineno": 305
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_pending_review",
          "type": "TestCaseFunction",
          "lineno": 235
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_users",
          "type": "TestCaseFunction",
          "lineno": 197
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_issue_comment_faster",
          "type": "TestCaseFunction",
          "lineno": 97
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_no_comments",
          "type": "TestCaseFunction",
          "lineno": 54
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_only_ignored_users",
          "type": "TestCaseFunction",
          "lineno": 265
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_pull_request_comment_faster",
          "type": "TestCaseFunction",
          "lineno": 128
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_pull_request_comment_ignore_before_ready",
          "type": "TestCaseFunction",
          "lineno": 159
        },
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_with_pull_request_comments",
          "type": "TestCaseFunction",
          "lineno": 69
        }
      ]
    },
    {
      "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse::test_get_stats_time_to_first_response",
          "type": "TestCaseFunction",
          "lineno": 379
        },
        {
          "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse::test_get_stats_time_to_first_response_with_all_none",
          "type": "TestCaseFunction",
          "lineno": 411
        }
      ]
    },
    {
      "nodeid": "test_time_to_first_response.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse",
          "type": "UnitTestCase"
        },
        {
          "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_created_at",
          "type": "TestCaseFunction",
          "lineno": 33
        },
        {
          "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_ready_for_review",
          "type": "TestCaseFunction",
          "lineno": 21
        },
        {
          "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_returns_none",
          "type": "TestCaseFunction",
          "lineno": 45
        }
      ]
    },
    {
      "nodeid": "test_time_to_merge.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_get_time_to_ready_for_review_event",
          "type": "TestCaseFunction",
          "lineno": 32
        },
        {
          "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_get_time_to_ready_for_review_no_event",
          "type": "TestCaseFunction",
          "lineno": 46
        },
        {
          "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_time_to_ready_for_review_draft",
          "type": "TestCaseFunction",
          "lineno": 22
        }
      ]
    },
    {
      "nodeid": "test_time_to_ready_for_review.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": ".",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "docs",
          "type": "Dir"
        },
        {
          "nodeid": "test_auth.py",
          "type": "Module"
        },
        {
          "nodeid": "test_config.py",
          "type": "Module"
        },
        {
          "nodeid": "test_config_get_bool.py",
          "type": "Module"
        },
        {
          "nodeid": "test_discussions.py",
          "type": "Module"
        },
        {
          "nodeid": "test_issue_metrics.py",
          "type": "Module"
        },
        {
          "nodeid": "test_json_writer.py",
          "type": "Module"
        },
        {
          "nodeid": "test_labels.py",
          "type": "Module"
        },
        {
          "nodeid": "test_markdown_helpers.py",
          "type": "Module"
        },
        {
          "nodeid": "test_markdown_writer.py",
          "type": "Module"
        },
        {
          "nodeid": "test_most_active_mentors.py",
          "type": "Module"
        },
        {
          "nodeid": "test_search.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_in_draft.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_to_answer.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_to_close.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_to_first_response.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_to_merge.py",
          "type": "Module"
        },
        {
          "nodeid": "test_time_to_ready_for_review.py",
          "type": "Module"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_invalid_credentials",
      "lineno": 111,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_invalid_credentials",
        "__wrapped__",
        "patchings",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_ghe",
      "lineno": 48,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_with_ghe",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_ghe_and_ghe_app",
      "lineno": 58,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_with_ghe_and_ghe_app",
        "__wrapped__",
        "patchings",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_github_app",
      "lineno": 21,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_with_github_app",
        "__wrapped__",
        "patchings",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_with_token",
      "lineno": 32,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_with_token",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_auth_to_github_without_authentication_information",
      "lineno": 40,
      "outcome": "passed",
      "keywords": [
        "test_auth_to_github_without_authentication_information",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_get_github_app_installation_token",
      "lineno": 72,
      "outcome": "passed",
      "keywords": [
        "test_get_github_app_installation_token",
        "__wrapped__",
        "patchings",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_auth.py::TestAuthToGithub::test_get_github_app_installation_token_request_failure",
      "lineno": 91,
      "outcome": "passed",
      "keywords": [
        "test_get_github_app_installation_token_request_failure",
        "__wrapped__",
        "patchings",
        "TestAuthToGithub",
        "test_auth.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Request failed: Request failed\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var",
      "lineno": 32,
      "outcome": "failed",
      "keywords": [
        "test_get_int_env_var",
        "__wrapped__",
        "TestGetIntFromEnv",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 163,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 38,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 163,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config.TestGetIntFromEnv testMethod=test_get_int_env_var>\n\n    @patch.dict(os.environ, {\"INT_ENV_VAR\": \"12345\"})\n    def test_get_int_env_var(self):\n        \"\"\"\n        Test that get_int_env_var returns the expected integer value.\n        \"\"\"\n>       result = get_int_env_var(\"INT_ENV_VAR\")\n\ntest_config.py:38: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'INT_ENV_VAR'\n\n    def get_int_env_var(env_var_name: str) -> int | None:\n        \"\"\"Get an integer environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n    \n        Returns:\n            The value of the environment variable as an integer or None.\n        \"\"\"\n>       env_var = os.environ.get(env_var_name)\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:163: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var_with_empty_env_var",
      "lineno": 40,
      "outcome": "failed",
      "keywords": [
        "test_get_int_env_var_with_empty_env_var",
        "__wrapped__",
        "TestGetIntFromEnv",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 163,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 48,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 163,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config.TestGetIntFromEnv testMethod=test_get_int_env_var_with_empty_env_var>\n\n    @patch.dict(os.environ, {\"INT_ENV_VAR\": \"\"})\n    def test_get_int_env_var_with_empty_env_var(self):\n        \"\"\"\n        This test verifies that the get_int_env_var function returns None\n        when the environment variable is empty.\n    \n        \"\"\"\n>       result = get_int_env_var(\"INT_ENV_VAR\")\n\ntest_config.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'INT_ENV_VAR'\n\n    def get_int_env_var(env_var_name: str) -> int | None:\n        \"\"\"Get an integer environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n    \n        Returns:\n            The value of the environment variable as an integer or None.\n        \"\"\"\n>       env_var = os.environ.get(env_var_name)\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:163: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetIntFromEnv::test_get_int_env_var_with_non_integer",
      "lineno": 50,
      "outcome": "failed",
      "keywords": [
        "test_get_int_env_var_with_non_integer",
        "__wrapped__",
        "TestGetIntFromEnv",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 163,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 58,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 163,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config.TestGetIntFromEnv testMethod=test_get_int_env_var_with_non_integer>\n\n    @patch.dict(os.environ, {\"INT_ENV_VAR\": \"not_an_int\"})\n    def test_get_int_env_var_with_non_integer(self):\n        \"\"\"\n        Test that get_int_env_var returns None when the environment variable is\n        a non-integer.\n    \n        \"\"\"\n>       result = get_int_env_var(\"INT_ENV_VAR\")\n\ntest_config.py:58: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'INT_ENV_VAR'\n\n    def get_int_env_var(env_var_name: str) -> int | None:\n        \"\"\"Get an integer environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n    \n        Returns:\n            The value of the environment variable as an integer or None.\n        \"\"\"\n>       env_var = os.environ.get(env_var_name)\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:163: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_auth_with_github_app_installation_missing_inputs",
      "lineno": 329,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars_auth_with_github_app_installation_missing_inputs",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\case.py",
          "lineno": 715,
          "message": "AssertionError: 'SEARCH_QUERY environment variable not set' != 'GH_APP_ID set and GH_APP_INSTALLATION_ID [34 chars] set'\n- SEARCH_QUERY environment variable not set\n+ GH_APP_ID set and GH_APP_INSTALLATION_ID or GH_APP_PRIVATE_KEY variable not set"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 347,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <test_config.TestGetEnvVars testMethod=test_get_env_vars_auth_with_github_app_installation_missing_inputs>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"ORGANIZATION\": \"my_organization\",\n            \"GH_APP_ID\": \"12345\",\n            \"GH_APP_INSTALLATION_ID\": \"\",\n            \"GH_APP_PRIVATE_KEY\": \"\",\n            \"GH_TOKEN\": \"\",\n            \"SEARCH_QUERY\": SEARCH_QUERY,\n        },\n        clear=True,\n    )\n    def test_get_env_vars_auth_with_github_app_installation_missing_inputs(self):\n        \"\"\"Test that an error is raised there are missing inputs for the gh app\"\"\"\n        with self.assertRaises(ValueError) as context_manager:\n            get_env_vars(True)\n        the_exception = context_manager.exception\n>       self.assertEqual(\n            str(the_exception),\n            \"GH_APP_ID set and GH_APP_INSTALLATION_ID or GH_APP_PRIVATE_KEY variable not set\",\n        )\nE       AssertionError: 'SEARCH_QUERY environment variable not set' != 'GH_APP_ID set and GH_APP_INSTALLATION_ID [34 chars] set'\nE       - SEARCH_QUERY environment variable not set\nE       + GH_APP_ID set and GH_APP_INSTALLATION_ID or GH_APP_PRIVATE_KEY variable not set\n\ntest_config.py:347: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_missing_query",
      "lineno": 214,
      "outcome": "passed",
      "keywords": [
        "test_get_env_vars_missing_query",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_missing_token",
      "lineno": 197,
      "outcome": "passed",
      "keywords": [
        "test_get_env_vars_missing_token",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_optional_values",
      "lineno": 232,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars_optional_values",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 285,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_config.TestGetEnvVars testMethod=test_get_env_vars_optional_values>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_APP_ID\": \"\",\n            \"GH_APP_INSTALLATION_ID\": \"\",\n            \"GH_APP_PRIVATE_KEY\": \"\",\n            \"GH_TOKEN\": TOKEN,\n            \"GH_ENTERPRISE_URL\": \"\",\n            \"HIDE_AUTHOR\": \"true\",\n            \"HIDE_ITEMS_CLOSED_COUNT\": \"true\",\n            \"HIDE_LABEL_METRICS\": \"true\",\n            \"HIDE_TIME_TO_ANSWER\": \"true\",\n            \"HIDE_TIME_TO_CLOSE\": \"true\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"true\",\n            \"IGNORE_USERS\": \"\",\n            \"LABELS_TO_MEASURE\": \"waiting-for-review,waiting-for-manager\",\n            \"NON_MENTIONING_LINKS\": \"true\",\n            \"OUTPUT_FILE\": \"issue_metrics.md\",\n            \"REPORT_TITLE\": \"Issue Metrics\",\n            \"SEARCH_QUERY\": SEARCH_QUERY,\n            \"RATE_LIMIT_BYPASS\": \"true\",\n            \"DRAFT_PR_TRACKING\": \"True\",\n        },\n    )\n    def test_get_env_vars_optional_values(self):\n        \"\"\"Test that optional values are set to their default values if not provided\"\"\"\n        expected_result = EnvVars(\n            gh_app_id=None,\n            gh_app_installation_id=None,\n            gh_app_private_key_bytes=b\"\",\n            gh_app_enterprise_only=False,\n            gh_token=TOKEN,\n            ghe=\"\",\n            hide_author=True,\n            hide_items_closed_count=True,\n            hide_label_metrics=True,\n            hide_time_to_answer=True,\n            hide_time_to_close=True,\n            hide_time_to_first_response=True,\n            ignore_user=[],\n            labels_to_measure=[\"waiting-for-review\", \"waiting-for-manager\"],\n            enable_mentor_count=False,\n            min_mentor_comments=10,\n            max_comments_eval=20,\n            heavily_involved_cutoff=3,\n            search_query=SEARCH_QUERY,\n            non_mentioning_links=True,\n            report_title=\"Issue Metrics\",\n            output_file=\"issue_metrics.md\",\n            rate_limit_bypass=True,\n            draft_pr_tracking=True,\n        )\n>       result = get_env_vars(True)\n\ntest_config.py:285: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_optionals_are_defaulted",
      "lineno": 287,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars_optionals_are_defaulted",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 327,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_config.TestGetEnvVars testMethod=test_get_env_vars_optionals_are_defaulted>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_APP_ID\": \"\",\n            \"GH_APP_INSTALLATION_ID\": \"\",\n            \"GH_APP_PRIVATE_KEY\": \"\",\n            \"GH_TOKEN\": \"TOKEN\",\n            \"SEARCH_QUERY\": SEARCH_QUERY,\n        },\n        clear=True,\n    )\n    def test_get_env_vars_optionals_are_defaulted(self):\n        \"\"\"Test that optional values are set to their default values if not provided\"\"\"\n        expected_result = EnvVars(\n            gh_app_id=None,\n            gh_app_installation_id=None,\n            gh_app_private_key_bytes=b\"\",\n            gh_app_enterprise_only=False,\n            gh_token=\"TOKEN\",\n            ghe=\"\",\n            hide_author=False,\n            hide_items_closed_count=False,\n            hide_label_metrics=False,\n            hide_time_to_answer=False,\n            hide_time_to_close=False,\n            hide_time_to_first_response=False,\n            ignore_user=[],\n            labels_to_measure=[],\n            enable_mentor_count=False,\n            min_mentor_comments=\"10\",\n            max_comments_eval=\"20\",\n            heavily_involved_cutoff=\"3\",\n            search_query=SEARCH_QUERY,\n            non_mentioning_links=False,\n            report_title=\"Issue Metrics\",\n            output_file=\"\",\n            rate_limit_bypass=False,\n            draft_pr_tracking=False,\n        )\n>       result = get_env_vars(True)\n\ntest_config.py:327: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_with_github_app",
      "lineno": 91,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars_with_github_app",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 143,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_config.TestGetEnvVars testMethod=test_get_env_vars_with_github_app>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_APP_ID\": \"12345\",\n            \"GH_APP_INSTALLATION_ID\": \"678910\",\n            \"GH_APP_PRIVATE_KEY\": \"hello\",\n            \"GH_TOKEN\": \"\",\n            \"GH_ENTERPRISE_URL\": \"\",\n            \"HIDE_AUTHOR\": \"\",\n            \"HIDE_ITEMS_CLOSED_COUNT\": \"false\",\n            \"HIDE_LABEL_METRICS\": \"\",\n            \"HIDE_TIME_TO_ANSWER\": \"\",\n            \"HIDE_TIME_TO_CLOSE\": \"\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"\",\n            \"IGNORE_USERS\": \"\",\n            \"LABELS_TO_MEASURE\": \"\",\n            \"NON_MENTIONING_LINKS\": \"false\",\n            \"OUTPUT_FILE\": \"\",\n            \"REPORT_TITLE\": \"\",\n            \"SEARCH_QUERY\": SEARCH_QUERY,\n            \"RATE_LIMIT_BYPASS\": \"false\",\n        },\n        clear=True,\n    )\n    def test_get_env_vars_with_github_app(self):\n        \"\"\"Test that all environment variables are set correctly using GitHub App\"\"\"\n        expected_result = EnvVars(\n            gh_app_id=12345,\n            gh_app_installation_id=678910,\n            gh_app_private_key_bytes=b\"hello\",\n            gh_app_enterprise_only=False,\n            gh_token=\"\",\n            ghe=\"\",\n            hide_author=False,\n            hide_items_closed_count=False,\n            hide_label_metrics=False,\n            hide_time_to_answer=False,\n            hide_time_to_close=False,\n            hide_time_to_first_response=False,\n            ignore_user=[],\n            labels_to_measure=[],\n            enable_mentor_count=False,\n            min_mentor_comments=\"10\",\n            max_comments_eval=\"20\",\n            heavily_involved_cutoff=\"3\",\n            search_query=SEARCH_QUERY,\n            non_mentioning_links=False,\n            report_title=\"\",\n            output_file=\"\",\n            draft_pr_tracking=False,\n        )\n>       result = get_env_vars(True)\n\ntest_config.py:143: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config.py::TestGetEnvVars::test_get_env_vars_with_token",
      "lineno": 145,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars_with_token",
        "__wrapped__",
        "TestGetEnvVars",
        "test_config.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_config.py",
            "lineno": 195,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_config.TestGetEnvVars testMethod=test_get_env_vars_with_token>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_APP_ID\": \"\",\n            \"GH_APP_INSTALLATION_ID\": \"\",\n            \"GH_APP_PRIVATE_KEY\": \"\",\n            \"GH_ENTERPRISE_URL\": \"\",\n            \"GH_TOKEN\": TOKEN,\n            \"HIDE_AUTHOR\": \"\",\n            \"HIDE_ITEMS_CLOSED_COUNT\": \"false\",\n            \"HIDE_LABEL_METRICS\": \"\",\n            \"HIDE_TIME_TO_ANSWER\": \"\",\n            \"HIDE_TIME_TO_CLOSE\": \"\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"\",\n            \"IGNORE_USERS\": \"\",\n            \"LABELS_TO_MEASURE\": \"\",\n            \"NON_MENTIONING_LINKS\": \"false\",\n            \"OUTPUT_FILE\": \"\",\n            \"REPORT_TITLE\": \"\",\n            \"SEARCH_QUERY\": SEARCH_QUERY,\n        },\n        clear=True,\n    )\n    def test_get_env_vars_with_token(self):\n        \"\"\"Test that all environment variables are set correctly using a list of repositories\"\"\"\n        expected_result = EnvVars(\n            gh_app_id=None,\n            gh_app_installation_id=None,\n            gh_app_private_key_bytes=b\"\",\n            gh_app_enterprise_only=False,\n            gh_token=TOKEN,\n            ghe=\"\",\n            hide_author=False,\n            hide_items_closed_count=False,\n            hide_label_metrics=False,\n            hide_time_to_answer=False,\n            hide_time_to_close=False,\n            hide_time_to_first_response=False,\n            ignore_user=[],\n            labels_to_measure=[],\n            enable_mentor_count=False,\n            min_mentor_comments=\"10\",\n            max_comments_eval=\"20\",\n            heavily_involved_cutoff=\"3\",\n            search_query=SEARCH_QUERY,\n            non_mentioning_links=False,\n            report_title=\"\",\n            output_file=\"\",\n        )\n>       result = get_env_vars(True)\n\ntest_config.py:195: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_does_not_exist_and_default_value_returns_false",
      "lineno": 64,
      "outcome": "failed",
      "keywords": [
        "test_get_bool_env_var_that_does_not_exist_and_default_value_returns_false",
        "__wrapped__",
        "TestEnv",
        "test_config_get_bool.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 148,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config_get_bool.py",
            "lineno": 76,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 148,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config_get_bool.TestEnv testMethod=test_get_bool_env_var_that_does_not_exist_and_default_value_returns_false>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_BOOL\": \"true\",\n        },\n        clear=True,\n    )\n    def test_get_bool_env_var_that_does_not_exist_and_default_value_returns_false(self):\n        \"\"\"Test that gets a boolean environment variable that does not exist\n        and default value returns: false\n        \"\"\"\n>       result = get_bool_env_var(\"DOES_NOT_EXIST\", False)\n\ntest_config_get_bool.py:76: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'DOES_NOT_EXIST', default = False\n\n    def get_bool_env_var(env_var_name: str, default: bool = False) -> bool:\n        \"\"\"Get a boolean environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n            default: The default value to return if the environment variable is not set.\n    \n        Returns:\n            The value of the environment variable as a boolean.\n        \"\"\"\n>       ev = os.environ.get(env_var_name, \"\")\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:148: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_does_not_exist_and_default_value_returns_true",
      "lineno": 50,
      "outcome": "failed",
      "keywords": [
        "test_get_bool_env_var_that_does_not_exist_and_default_value_returns_true",
        "__wrapped__",
        "TestEnv",
        "test_config_get_bool.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 148,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config_get_bool.py",
            "lineno": 62,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 148,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config_get_bool.TestEnv testMethod=test_get_bool_env_var_that_does_not_exist_and_default_value_returns_true>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_BOOL\": \"false\",\n        },\n        clear=True,\n    )\n    def test_get_bool_env_var_that_does_not_exist_and_default_value_returns_true(self):\n        \"\"\"Test that gets a boolean environment variable that does not exist\n        and default value returns: true\n        \"\"\"\n>       result = get_bool_env_var(\"DOES_NOT_EXIST\", True)\n\ntest_config_get_bool.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'DOES_NOT_EXIST', default = True\n\n    def get_bool_env_var(env_var_name: str, default: bool = False) -> bool:\n        \"\"\"Get a boolean environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n            default: The default value to return if the environment variable is not set.\n    \n        Returns:\n            The value of the environment variable as a boolean.\n        \"\"\"\n>       ev = os.environ.get(env_var_name, \"\")\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:148: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_false",
      "lineno": 24,
      "outcome": "failed",
      "keywords": [
        "test_get_bool_env_var_that_exists_and_is_false",
        "__wrapped__",
        "TestEnv",
        "test_config_get_bool.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 148,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config_get_bool.py",
            "lineno": 34,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 148,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config_get_bool.TestEnv testMethod=test_get_bool_env_var_that_exists_and_is_false>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_BOOL\": \"false\",\n        },\n        clear=True,\n    )\n    def test_get_bool_env_var_that_exists_and_is_false(self):\n        \"\"\"Test that gets a boolean environment variable that exists and is false\"\"\"\n>       result = get_bool_env_var(\"TEST_BOOL\", False)\n\ntest_config_get_bool.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'TEST_BOOL', default = False\n\n    def get_bool_env_var(env_var_name: str, default: bool = False) -> bool:\n        \"\"\"Get a boolean environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n            default: The default value to return if the environment variable is not set.\n    \n        Returns:\n            The value of the environment variable as a boolean.\n        \"\"\"\n>       ev = os.environ.get(env_var_name, \"\")\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:148: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_false_due_to_invalid_value",
      "lineno": 36,
      "outcome": "failed",
      "keywords": [
        "test_get_bool_env_var_that_exists_and_is_false_due_to_invalid_value",
        "__wrapped__",
        "TestEnv",
        "test_config_get_bool.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 148,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config_get_bool.py",
            "lineno": 48,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 148,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config_get_bool.TestEnv testMethod=test_get_bool_env_var_that_exists_and_is_false_due_to_invalid_value>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_BOOL\": \"nope\",\n        },\n        clear=True,\n    )\n    def test_get_bool_env_var_that_exists_and_is_false_due_to_invalid_value(self):\n        \"\"\"Test that gets a boolean environment variable that exists and is false\n        due to an invalid value\n        \"\"\"\n>       result = get_bool_env_var(\"TEST_BOOL\", False)\n\ntest_config_get_bool.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'TEST_BOOL', default = False\n\n    def get_bool_env_var(env_var_name: str, default: bool = False) -> bool:\n        \"\"\"Get a boolean environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n            default: The default value to return if the environment variable is not set.\n    \n        Returns:\n            The value of the environment variable as a boolean.\n        \"\"\"\n>       ev = os.environ.get(env_var_name, \"\")\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:148: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_config_get_bool.py::TestEnv::test_get_bool_env_var_that_exists_and_is_true",
      "lineno": 12,
      "outcome": "failed",
      "keywords": [
        "test_get_bool_env_var_that_exists_and_is_true",
        "__wrapped__",
        "TestEnv",
        "test_config_get_bool.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 148,
          "message": "NameError: name 'os' is not defined. Did you forget to import 'os'"
        },
        "traceback": [
          {
            "path": "test_config_get_bool.py",
            "lineno": 22,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 148,
            "message": "NameError"
          }
        ],
        "longrepr": "self = <test_config_get_bool.TestEnv testMethod=test_get_bool_env_var_that_exists_and_is_true>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_BOOL\": \"true\",\n        },\n        clear=True,\n    )\n    def test_get_bool_env_var_that_exists_and_is_true(self):\n        \"\"\"Test that gets a boolean environment variable that exists and is true\"\"\"\n>       result = get_bool_env_var(\"TEST_BOOL\", False)\n\ntest_config_get_bool.py:22: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nenv_var_name = 'TEST_BOOL', default = False\n\n    def get_bool_env_var(env_var_name: str, default: bool = False) -> bool:\n        \"\"\"Get a boolean environment variable.\n    \n        Args:\n            env_var_name: The name of the environment variable to retrieve.\n            default: The default value to return if the environment variable is not set.\n    \n        Returns:\n            The value of the environment variable as a boolean.\n        \"\"\"\n>       ev = os.environ.get(env_var_name, \"\")\nE       NameError: name 'os' is not defined. Did you forget to import 'os'\n\nconfig.py:148: NameError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_error_status_code",
      "lineno": 116,
      "outcome": "passed",
      "keywords": [
        "test_get_discussions_error_status_code",
        "__wrapped__",
        "patchings",
        "TestGetDiscussions",
        "test_discussions.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_graphql_error",
      "lineno": 128,
      "outcome": "passed",
      "keywords": [
        "test_get_discussions_graphql_error",
        "__wrapped__",
        "patchings",
        "TestGetDiscussions",
        "test_discussions.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_multiple_pages",
      "lineno": 69,
      "outcome": "passed",
      "keywords": [
        "test_get_discussions_multiple_pages",
        "__wrapped__",
        "patchings",
        "TestGetDiscussions",
        "test_discussions.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_discussions.py::TestGetDiscussions::test_get_discussions_single_page",
      "lineno": 29,
      "outcome": "passed",
      "keywords": [
        "test_get_discussions_single_page",
        "__wrapped__",
        "patchings",
        "TestGetDiscussions",
        "test_discussions.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetEnvVars::test_get_env_vars",
      "lineno": 30,
      "outcome": "failed",
      "keywords": [
        "test_get_env_vars",
        "__wrapped__",
        "TestGetEnvVars",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 39,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestGetEnvVars testMethod=test_get_env_vars>\n\n    @patch.dict(\n        os.environ,\n        {\"GH_TOKEN\": \"test_token\", \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\"},\n    )\n    def test_get_env_vars(self):\n        \"\"\"Test that the function correctly retrieves the environment variables.\"\"\"\n    \n        # Call the function and check the result\n>       search_query = get_env_vars(test=True).search_query\n\ntest_issue_metrics.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetEnvVars::test_get_env_vars_missing_query",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_get_env_vars_missing_query",
        "TestGetEnvVars",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_with_hide_envs",
      "lineno": 59,
      "outcome": "failed",
      "keywords": [
        "test_get_per_issue_metrics_with_hide_envs",
        "__wrapped__",
        "TestGetPerIssueMetrics",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 126,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestGetPerIssueMetrics testMethod=test_get_per_issue_metrics_with_hide_envs>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_TOKEN\": \"test_token\",\n            \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\",\n            \"HIDE_AUTHOR\": \"true\",\n            \"HIDE_LABEL_METRICS\": \"true\",\n            \"HIDE_TIME_TO_ANSWER\": \"true\",\n            \"HIDE_TIME_TO_CLOSE\": \"true\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"true\",\n        },\n    )\n    def test_get_per_issue_metrics_with_hide_envs(self):\n        \"\"\"\n        Test that the function correctly calculates the metrics for\n        a list of GitHub issues where HIDE_* envs are set true\n        \"\"\"\n    \n        # Create mock data\n        mock_issue1 = MagicMock(\n            title=\"Issue 1\",\n            html_url=\"https://github.com/user/repo/issues/1\",\n            user={\"login\": \"alice\"},\n            state=\"open\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n        )\n    \n        mock_comment1 = MagicMock()\n        mock_comment1.created_at = datetime.fromisoformat(\"2023-01-02T00:00:00Z\")\n        mock_issue1.issue.comments.return_value = [mock_comment1]\n        mock_issue1.issue.pull_request_urls = None\n    \n        mock_issue2 = MagicMock(\n            title=\"Issue 2\",\n            html_url=\"https://github.com/user/repo/issues/2\",\n            user={\"login\": \"bob\"},\n            state=\"closed\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n            closed_at=\"2023-01-04T00:00:00Z\",\n        )\n    \n        mock_comment2 = MagicMock()\n        mock_comment2.created_at = datetime.fromisoformat(\"2023-01-03T00:00:00Z\")\n        mock_issue2.issue.comments.return_value = [mock_comment2]\n        mock_issue2.issue.pull_request_urls = None\n    \n        issues = [\n            mock_issue1,\n            mock_issue2,\n        ]\n    \n        # Call the function and check the result\n        with unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_first_response\",\n            measure_time_to_first_response,\n        ), unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_close\", measure_time_to_close\n        ):\n            (\n                result_issues_with_metrics,\n                result_num_issues_open,\n                result_num_issues_closed,\n            ) = get_per_issue_metrics(\n                issues,\n>               env_vars=get_env_vars(test=True),\n            )\n\ntest_issue_metrics.py:126: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_with_ignore_users",
      "lineno": 277,
      "outcome": "failed",
      "keywords": [
        "test_get_per_issue_metrics_with_ignore_users",
        "__wrapped__",
        "TestGetPerIssueMetrics",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 340,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestGetPerIssueMetrics testMethod=test_get_per_issue_metrics_with_ignore_users>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_TOKEN\": \"test_token\",\n            \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\",\n            \"IGNORE_USERS\": \"alice\",\n        },\n    )\n    def test_get_per_issue_metrics_with_ignore_users(self):\n        \"\"\"\n        Test that the function correctly filters out issues\n        with authors in the IGNORE_USERS variable\n        \"\"\"\n    \n        # Create mock data\n        mock_issue1 = MagicMock(\n            title=\"Issue 1\",\n            html_url=\"https://github.com/user/repo/issues/1\",\n            user={\"login\": \"alice\"},\n            state=\"open\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n        )\n    \n        mock_comment1 = MagicMock()\n        mock_comment1.created_at = datetime.fromisoformat(\"2023-01-02T00:00:00Z\")\n        mock_issue1.issue.comments.return_value = [mock_comment1]\n        mock_issue1.issue.pull_request_urls = None\n    \n        mock_issue2 = MagicMock(\n            title=\"Issue 2\",\n            html_url=\"https://github.com/user/repo/issues/2\",\n            user={\"login\": \"bob\"},\n            state=\"closed\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n            closed_at=\"2023-01-04T00:00:00Z\",\n        )\n    \n        mock_comment2 = MagicMock()\n        mock_comment2.created_at = datetime.fromisoformat(\"2023-01-03T00:00:00Z\")\n        mock_issue2.issue.comments.return_value = [mock_comment2]\n        mock_issue2.issue.pull_request_urls = None\n    \n        issues = [\n            mock_issue1,\n            mock_issue2,\n        ]\n    \n        # Call the function and check the result\n        with unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_first_response\",\n            measure_time_to_first_response,\n        ), unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_close\", measure_time_to_close\n        ):\n            (\n                result_issues_with_metrics,\n                result_num_issues_open,\n                result_num_issues_closed,\n            ) = get_per_issue_metrics(\n                issues,\n>               env_vars=get_env_vars(test=True),\n                ignore_users=[\"alice\"],\n            )\n\ntest_issue_metrics.py:340: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestGetPerIssueMetrics::test_get_per_issue_metrics_without_hide_envs",
      "lineno": 168,
      "outcome": "failed",
      "keywords": [
        "test_get_per_issue_metrics_without_hide_envs",
        "__wrapped__",
        "TestGetPerIssueMetrics",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 235,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestGetPerIssueMetrics testMethod=test_get_per_issue_metrics_without_hide_envs>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_TOKEN\": \"test_token\",\n            \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\",\n            \"HIDE_AUTHOR\": \"false\",\n            \"HIDE_LABEL_METRICS\": \"false\",\n            \"HIDE_TIME_TO_ANSWER\": \"false\",\n            \"HIDE_TIME_TO_CLOSE\": \"false\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"false\",\n        },\n    )\n    def test_get_per_issue_metrics_without_hide_envs(self):\n        \"\"\"\n        Test that the function correctly calculates the metrics for\n        a list of GitHub issues where HIDE_* envs are set false\n        \"\"\"\n    \n        # Create mock data\n        mock_issue1 = MagicMock(\n            title=\"Issue 1\",\n            html_url=\"https://github.com/user/repo/issues/1\",\n            user={\"login\": \"alice\"},\n            state=\"open\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n        )\n    \n        mock_comment1 = MagicMock()\n        mock_comment1.created_at = datetime.fromisoformat(\"2023-01-02T00:00:00Z\")\n        mock_issue1.issue.comments.return_value = [mock_comment1]\n        mock_issue1.issue.pull_request_urls = None\n    \n        mock_issue2 = MagicMock(\n            title=\"Issue 2\",\n            html_url=\"https://github.com/user/repo/issues/2\",\n            user={\"login\": \"bob\"},\n            state=\"closed\",\n            comments=1,\n            created_at=\"2023-01-01T00:00:00Z\",\n            closed_at=\"2023-01-04T00:00:00Z\",\n        )\n    \n        mock_comment2 = MagicMock()\n        mock_comment2.created_at = datetime.fromisoformat(\"2023-01-03T00:00:00Z\")\n        mock_issue2.issue.comments.return_value = [mock_comment2]\n        mock_issue2.issue.pull_request_urls = None\n    \n        issues = [\n            mock_issue1,\n            mock_issue2,\n        ]\n    \n        # Call the function and check the result\n        with unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_first_response\",\n            measure_time_to_first_response,\n        ), unittest.mock.patch(  # type:ignore\n            \"issue_metrics.measure_time_to_close\", measure_time_to_close\n        ):\n            (\n                result_issues_with_metrics,\n                result_num_issues_open,\n                result_num_issues_closed,\n            ) = get_per_issue_metrics(\n                issues,\n>               env_vars=get_env_vars(test=True),\n            )\n\ntest_issue_metrics.py:235: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestDiscussionMetrics::test_get_per_issue_metrics_with_discussion",
      "lineno": 402,
      "outcome": "failed",
      "keywords": [
        "test_get_per_issue_metrics_with_discussion",
        "__wrapped__",
        "TestDiscussionMetrics",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 415,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestDiscussionMetrics testMethod=test_get_per_issue_metrics_with_discussion>\n\n    @patch.dict(\n        os.environ,\n        {\"GH_TOKEN\": \"test_token\", \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\"},\n    )\n    def test_get_per_issue_metrics_with_discussion(self):\n        \"\"\"\n        Test that the function correctly calculates\n        the metrics for a list of GitHub issues with discussions.\n        \"\"\"\n    \n        issues = [self.issue1, self.issue2]\n        metrics = get_per_issue_metrics(\n>           issues, discussions=True, env_vars=get_env_vars(test=True)\n        )\n\ntest_issue_metrics.py:415: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_issue_metrics.py::TestDiscussionMetrics::test_get_per_issue_metrics_with_discussion_with_hide_envs",
      "lineno": 436,
      "outcome": "failed",
      "keywords": [
        "test_get_per_issue_metrics_with_discussion_with_hide_envs",
        "__wrapped__",
        "TestDiscussionMetrics",
        "test_issue_metrics.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_issue_metrics.py",
            "lineno": 458,
            "message": ""
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_issue_metrics.TestDiscussionMetrics testMethod=test_get_per_issue_metrics_with_discussion_with_hide_envs>\n\n    @patch.dict(\n        os.environ,\n        {\n            \"GH_TOKEN\": \"test_token\",\n            \"SEARCH_QUERY\": \"is:issue is:open repo:user/repo\",\n            \"HIDE_AUTHOR\": \"true\",\n            \"HIDE_LABEL_METRICS\": \"true\",\n            \"HIDE_TIME_TO_ANSWER\": \"true\",\n            \"HIDE_TIME_TO_CLOSE\": \"true\",\n            \"HIDE_TIME_TO_FIRST_RESPONSE\": \"true\",\n        },\n    )\n    def test_get_per_issue_metrics_with_discussion_with_hide_envs(self):\n        \"\"\"\n        Test that the function correctly calculates\n        the metrics for a list of GitHub issues with discussions\n        and HIDE_* env vars set to True\n        \"\"\"\n    \n        issues = [self.issue1, self.issue2]\n        metrics = get_per_issue_metrics(\n>           issues, discussions=True, env_vars=get_env_vars(test=True)\n        )\n\ntest_issue_metrics.py:458: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = True\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_json_writer.py::TestWriteToJson::test_write_to_json",
      "lineno": 16,
      "outcome": "passed",
      "keywords": [
        "test_write_to_json",
        "TestWriteToJson",
        "test_json_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_json_writer.py::TestWriteToJson::test_write_to_json_with_no_response",
      "lineno": 134,
      "outcome": "passed",
      "keywords": [
        "test_write_to_json_with_no_response",
        "TestWriteToJson",
        "test_json_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_labels.py::TestLabels::test_get_label_events",
      "lineno": 50,
      "outcome": "passed",
      "keywords": [
        "test_get_label_events",
        "TestLabels",
        "test_labels.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_closed_issue",
      "lineno": 59,
      "outcome": "passed",
      "keywords": [
        "test_get_label_metrics_closed_issue",
        "TestLabels",
        "test_labels.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_closed_issue_labeled_past_closed_at",
      "lineno": 88,
      "outcome": "passed",
      "keywords": [
        "test_get_label_metrics_closed_issue_labeled_past_closed_at",
        "TestLabels",
        "test_labels.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_labels.py::TestLabels::test_get_label_metrics_open_issue",
      "lineno": 66,
      "outcome": "passed",
      "keywords": [
        "test_get_label_metrics_open_issue",
        "TestLabels",
        "test_labels.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_labels.py::TestGetAverageTimeInLabels::test_get_stats_time_in_labels",
      "lineno": 113,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_in_labels",
        "TestGetAverageTimeInLabels",
        "test_labels.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "{'avg': {'bug': datetime.timedelta(days=2), 'feature': None}, 'med': {'bug': datetime.timedelta(days=2), 'feature': None}, '90p': {'bug': datetime.timedelta(days=2), 'feature': None}}\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers::test_markdown_too_large_for_issue_body",
      "lineno": 13,
      "outcome": "passed",
      "keywords": [
        "test_markdown_too_large_for_issue_body",
        "TestMarkdownHelpers",
        "test_markdown_helpers.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_helpers.py::TestMarkdownHelpers::test_split_markdown_file",
      "lineno": 34,
      "outcome": "passed",
      "keywords": [
        "test_split_markdown_file",
        "TestMarkdownHelpers",
        "test_markdown_helpers.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown",
      "lineno": 31,
      "outcome": "failed",
      "keywords": [
        "test_write_to_markdown",
        "__wrapped__",
        "TestWriteToMarkdown",
        "test_markdown_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_markdown_writer.py",
            "lineno": 95,
            "message": ""
          },
          {
            "path": "markdown_writer.py",
            "lineno": 136,
            "message": "in write_to_markdown"
          },
          {
            "path": "markdown_writer.py",
            "lineno": 55,
            "message": "in get_non_hidden_columns"
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_markdown_writer.TestWriteToMarkdown testMethod=test_write_to_markdown>\n\n    def test_write_to_markdown(self):\n        \"\"\"Test that write_to_markdown writes the correct markdown file.\n    \n        This test creates a list of mock GitHub issues with time to first response\n        attributes, calls write_to_markdown with the list and the average time to\n        first response, time to close and checks that the function writes the correct\n        markdown file.\n    \n        \"\"\"\n        # Create mock data\n        issues_with_metrics = [\n            IssueWithMetrics(\n                title=\"Issue 1\",\n                html_url=\"https://github.com/user/repo/issues/1\",\n                author=\"alice\",\n                time_to_first_response=timedelta(days=1),\n                time_to_close=timedelta(days=2),\n                time_to_answer=timedelta(days=3),\n                time_in_draft=timedelta(days=1),\n                labels_metrics={\"bug\": timedelta(days=4)},\n            ),\n            IssueWithMetrics(\n                title=\"Issue 2\\r\",\n                html_url=\"https://github.com/user/repo/issues/2\",\n                author=\"bob\",\n                time_to_first_response=timedelta(days=3),\n                time_to_close=timedelta(days=4),\n                time_to_answer=timedelta(days=5),\n                time_in_draft=timedelta(days=1),\n                labels_metrics={\"bug\": timedelta(days=2)},\n            ),\n        ]\n        time_to_first_response = {\n            \"avg\": timedelta(days=2),\n            \"med\": timedelta(days=2),\n            \"90p\": timedelta(days=2),\n        }\n        time_to_close = {\n            \"avg\": timedelta(days=3),\n            \"med\": timedelta(days=3),\n            \"90p\": timedelta(days=3),\n        }\n        time_to_answer = {\n            \"avg\": timedelta(days=4),\n            \"med\": timedelta(days=4),\n            \"90p\": timedelta(days=4),\n        }\n        time_in_draft = {\n            \"avg\": timedelta(days=1),\n            \"med\": timedelta(days=1),\n            \"90p\": timedelta(days=1),\n        }\n        time_in_labels = {\n            \"avg\": {\"bug\": \"1 day, 12:00:00\"},\n            \"med\": {\"bug\": \"1 day, 12:00:00\"},\n            \"90p\": {\"bug\": \"1 day, 12:00:00\"},\n        }\n    \n        num_issues_opened = 2\n        num_issues_closed = 1\n        num_mentor_count = 5\n    \n        # Call the function\n>       write_to_markdown(\n            issues_with_metrics=issues_with_metrics,\n            average_time_to_first_response=time_to_first_response,\n            average_time_to_close=time_to_close,\n            average_time_to_answer=time_to_answer,\n            average_time_in_draft=time_in_draft,\n            average_time_in_labels=time_in_labels,\n            num_issues_opened=num_issues_opened,\n            num_issues_closed=num_issues_closed,\n            num_mentor_count=num_mentor_count,\n            labels=[\"bug\"],\n            search_query=\"is:issue is:open label:bug\",\n            report_title=\"Issue Metrics\",\n            output_file=\"issue_metrics.md\",\n            ghe=\"\",\n        )\n\ntest_markdown_writer.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nmarkdown_writer.py:136: in write_to_markdown\n    columns = get_non_hidden_columns(labels)\nmarkdown_writer.py:55: in get_non_hidden_columns\n    env_vars = get_env_vars()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = False\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown_no_issues",
      "lineno": 251,
      "outcome": "failed",
      "keywords": [
        "test_write_to_markdown_no_issues",
        "__wrapped__",
        "TestWriteToMarkdown",
        "test_markdown_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_markdown_writer.py",
            "lineno": 256,
            "message": ""
          },
          {
            "path": "markdown_writer.py",
            "lineno": 136,
            "message": "in write_to_markdown"
          },
          {
            "path": "markdown_writer.py",
            "lineno": 55,
            "message": "in get_non_hidden_columns"
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_markdown_writer.TestWriteToMarkdown testMethod=test_write_to_markdown_no_issues>\n\n    def test_write_to_markdown_no_issues(self):\n        \"\"\"Test that write_to_markdown writes the correct markdown file when no issues are found.\"\"\"\n        # Call the function with no issues\n        with patch(\"builtins.open\", mock_open()) as mock_open_file:\n>           write_to_markdown(\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n                None,\n                report_title=\"Issue Metrics\",\n            )\n\ntest_markdown_writer.py:256: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nmarkdown_writer.py:136: in write_to_markdown\n    columns = get_non_hidden_columns(labels)\nmarkdown_writer.py:55: in get_non_hidden_columns\n    env_vars = get_env_vars()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = False\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdown::test_write_to_markdown_with_vertical_bar_in_title",
      "lineno": 143,
      "outcome": "failed",
      "keywords": [
        "test_write_to_markdown_with_vertical_bar_in_title",
        "__wrapped__",
        "TestWriteToMarkdown",
        "test_markdown_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_markdown_writer.py",
            "lineno": 206,
            "message": ""
          },
          {
            "path": "markdown_writer.py",
            "lineno": 136,
            "message": "in write_to_markdown"
          },
          {
            "path": "markdown_writer.py",
            "lineno": 55,
            "message": "in get_non_hidden_columns"
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_markdown_writer.TestWriteToMarkdown testMethod=test_write_to_markdown_with_vertical_bar_in_title>\n\n    def test_write_to_markdown_with_vertical_bar_in_title(self):\n        \"\"\"Test that write_to_markdown writes the correct markdown file when the title contains a vertical bar.\n    \n        This test creates a list of mock GitHub issues (one of which contains a vertical\n        bar in the title) with time to first response attributes, calls write_to_markdown\n        with the list and the average time to first response, time to close and checks\n        that the function writes the correct markdown file.\n    \n        \"\"\"\n        # Create mock data\n        issues_with_metrics = [\n            IssueWithMetrics(\n                title=\"Issue 1\",\n                html_url=\"https://github.com/user/repo/issues/1\",\n                author=\"alice\",\n                time_to_first_response=timedelta(days=1),\n                time_to_close=timedelta(days=2),\n                time_to_answer=timedelta(days=3),\n                time_in_draft=timedelta(days=1),\n                labels_metrics={\"bug\": timedelta(days=1)},\n            ),\n            IssueWithMetrics(\n                title=\"feat| Issue 2\",  # title contains a vertical bar\n                html_url=\"https://github.com/user/repo/issues/2\",\n                author=\"bob\",\n                time_to_first_response=timedelta(days=3),\n                time_to_close=timedelta(days=4),\n                time_to_answer=timedelta(days=5),\n                labels_metrics={\"bug\": timedelta(days=2)},\n            ),\n        ]\n        average_time_to_first_response = {\n            \"avg\": timedelta(days=2),\n            \"med\": timedelta(days=2),\n            \"90p\": timedelta(days=2),\n        }\n        average_time_to_close = {\n            \"avg\": timedelta(days=3),\n            \"med\": timedelta(days=3),\n            \"90p\": timedelta(days=3),\n        }\n        average_time_to_answer = {\n            \"avg\": timedelta(days=4),\n            \"med\": timedelta(days=4),\n            \"90p\": timedelta(days=4),\n        }\n        average_time_in_draft = {\n            \"avg\": timedelta(days=1),\n            \"med\": timedelta(days=1),\n            \"90p\": timedelta(days=1),\n        }\n        average_time_in_labels = {\n            \"avg\": {\"bug\": \"1 day, 12:00:00\"},\n            \"med\": {\"bug\": \"1 day, 12:00:00\"},\n            \"90p\": {\"bug\": \"1 day, 12:00:00\"},\n        }\n    \n        num_issues_opened = 2\n        num_issues_closed = 1\n        num_mentor_count = 5\n    \n        # Call the function\n>       write_to_markdown(\n            issues_with_metrics=issues_with_metrics,\n            average_time_to_first_response=average_time_to_first_response,\n            average_time_to_close=average_time_to_close,\n            average_time_to_answer=average_time_to_answer,\n            average_time_in_draft=average_time_in_draft,\n            average_time_in_labels=average_time_in_labels,\n            num_issues_opened=num_issues_opened,\n            num_issues_closed=num_issues_closed,\n            num_mentor_count=num_mentor_count,\n            labels=[\"bug\"],\n            report_title=\"Issue Metrics\",\n            output_file=\"issue_metrics.md\",\n        )\n\ntest_markdown_writer.py:206: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nmarkdown_writer.py:136: in write_to_markdown\n    columns = get_non_hidden_columns(labels)\nmarkdown_writer.py:55: in get_non_hidden_columns\n    env_vars = get_env_vars()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = False\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_markdown_writer.py::TestWriteToMarkdownWithEnv::test_writes_markdown_file_with_non_hidden_columns_only",
      "lineno": 299,
      "outcome": "failed",
      "keywords": [
        "test_writes_markdown_file_with_non_hidden_columns_only",
        "__wrapped__",
        "TestWriteToMarkdownWithEnv",
        "test_markdown_writer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\config.py",
          "lineno": 180,
          "message": "ValueError: SEARCH_QUERY environment variable not set"
        },
        "traceback": [
          {
            "path": "test_markdown_writer.py",
            "lineno": 344,
            "message": ""
          },
          {
            "path": "markdown_writer.py",
            "lineno": 136,
            "message": "in write_to_markdown"
          },
          {
            "path": "markdown_writer.py",
            "lineno": 55,
            "message": "in get_non_hidden_columns"
          },
          {
            "path": "config.py",
            "lineno": 180,
            "message": "ValueError"
          }
        ],
        "longrepr": "self = <test_markdown_writer.TestWriteToMarkdownWithEnv testMethod=test_writes_markdown_file_with_non_hidden_columns_only>\n\n    def test_writes_markdown_file_with_non_hidden_columns_only(self):\n        \"\"\"\n        Test that write_to_markdown writes the correct\n        markdown file with non-hidden columns only.\n        \"\"\"\n    \n        # Create mock data\n        issues_with_metrics = [\n            IssueWithMetrics(\n                title=\"Issue 1\",\n                html_url=\"https://github.com/user/repo/issues/1\",\n                author=\"alice\",\n                time_to_first_response=timedelta(minutes=10),\n                time_to_close=timedelta(days=1),\n                time_to_answer=timedelta(hours=2),\n                time_in_draft=timedelta(days=1),\n                labels_metrics={\n                    \"label1\": timedelta(days=1),\n                },\n            ),\n            IssueWithMetrics(\n                title=\"Issue 2\",\n                html_url=\"https://github.com/user/repo/issues/2\",\n                author=\"bob\",\n                time_to_first_response=timedelta(minutes=20),\n                time_to_close=timedelta(days=2),\n                time_to_answer=timedelta(hours=4),\n                labels_metrics={\n                    \"label1\": timedelta(days=1),\n                },\n            ),\n        ]\n        average_time_to_first_response = timedelta(minutes=15)\n        average_time_to_close = timedelta(days=1.5)\n        average_time_to_answer = timedelta(hours=3)\n        average_time_in_draft = timedelta(days=1)\n        average_time_in_labels = {\n            \"label1\": timedelta(days=1),\n        }\n        num_issues_opened = 2\n        num_issues_closed = 2\n        num_mentor_count = 5\n    \n        # Call the function\n>       write_to_markdown(\n            issues_with_metrics=issues_with_metrics,\n            average_time_to_first_response=average_time_to_first_response,\n            average_time_to_close=average_time_to_close,\n            average_time_to_answer=average_time_to_answer,\n            average_time_in_labels=average_time_in_labels,\n            average_time_in_draft=average_time_in_draft,\n            num_issues_opened=num_issues_opened,\n            num_issues_closed=num_issues_closed,\n            num_mentor_count=num_mentor_count,\n            labels=[\"label1\"],\n            search_query=\"repo:user/repo is:issue\",\n            hide_label_metrics=True,\n            hide_items_closed_count=True,\n            non_mentioning_links=True,\n            report_title=\"Issue Metrics\",\n            output_file=\"issue_metrics.md\",\n        )\n\ntest_markdown_writer.py:344: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nmarkdown_writer.py:136: in write_to_markdown\n    columns = get_non_hidden_columns(labels)\nmarkdown_writer.py:55: in get_non_hidden_columns\n    env_vars = get_env_vars()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntest = False\n\n    def get_env_vars(test: bool = False) -> EnvVars:\n        \"\"\"\n        Get the environment variables for use in the script.\n    \n        Returns EnvVars object with all environment variables\n        \"\"\"\n        search_query = settings.get(\"SEARCH_QUERY\")\n        if not search_query:\n>           raise ValueError(\"SEARCH_QUERY environment variable not set\")\nE           ValueError: SEARCH_QUERY environment variable not set\n\nconfig.py:180: ValueError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_count_comments_per_user_limit",
      "lineno": 24,
      "outcome": "passed",
      "keywords": [
        "test_count_comments_per_user_limit",
        "TestCountCommentsPerUser",
        "test_most_active_mentors.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_count_comments_per_user_with_ignores",
      "lineno": 56,
      "outcome": "passed",
      "keywords": [
        "test_count_comments_per_user_with_ignores",
        "TestCountCommentsPerUser",
        "test_most_active_mentors.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_most_active_mentors.py::TestCountCommentsPerUser::test_get_mentor_count",
      "lineno": 95,
      "outcome": "passed",
      "keywords": [
        "test_get_mentor_count",
        "TestCountCommentsPerUser",
        "test_most_active_mentors.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_just_owner_or_org",
      "lineno": 49,
      "outcome": "passed",
      "keywords": [
        "test_search_issues_with_just_owner_or_org",
        "TestSearchIssues",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Searching for issues...\nIssue 1\nIssue 2\nIssue 3\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_just_owner_or_org_with_bypass",
      "lineno": 73,
      "outcome": "passed",
      "keywords": [
        "test_search_issues_with_just_owner_or_org_with_bypass",
        "TestSearchIssues",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Searching for issues...\nIssue 1\nIssue 2\nIssue 3\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestSearchIssues::test_search_issues_with_owner_and_repository",
      "lineno": 26,
      "outcome": "passed",
      "keywords": [
        "test_search_issues_with_owner_and_repository",
        "TestSearchIssues",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Searching for issues...\nIssue 1\nIssue 2\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owner_and_repositories_without_repo_in_query",
      "lineno": 122,
      "outcome": "passed",
      "keywords": [
        "test_get_owner_and_repositories_without_repo_in_query",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_multiple_entries",
      "lineno": 133,
      "outcome": "passed",
      "keywords": [
        "test_get_owners_and_repositories_with_multiple_entries",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_org",
      "lineno": 141,
      "outcome": "passed",
      "keywords": [
        "test_get_owners_and_repositories_with_org",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_with_user",
      "lineno": 147,
      "outcome": "passed",
      "keywords": [
        "test_get_owners_and_repositories_with_user",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_and_repositories_without_either_in_query",
      "lineno": 128,
      "outcome": "passed",
      "keywords": [
        "test_get_owners_and_repositories_without_either_in_query",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_search.py::TestGetOwnerAndRepository::test_get_owners_with_owner_and_repo_in_query",
      "lineno": 116,
      "outcome": "passed",
      "keywords": [
        "test_get_owners_with_owner_and_repo_in_query",
        "TestGetOwnerAndRepository",
        "test_search.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_multiple_intervals",
      "lineno": 57,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_multiple_intervals",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_no_draft_events",
      "lineno": 101,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_no_draft_events",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_ongoing_draft",
      "lineno": 83,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_ongoing_draft",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_with_ready_for_review",
      "lineno": 22,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_with_ready_for_review",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_without_ready_for_review",
      "lineno": 40,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_without_ready_for_review",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestMeasureTimeInDraft::test_time_in_draft_without_ready_for_review_and_closed",
      "lineno": 111,
      "outcome": "passed",
      "keywords": [
        "test_time_in_draft_without_ready_for_review_and_closed",
        "TestMeasureTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_empty_list",
      "lineno": 169,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_in_draft_empty_list",
        "TestGetStatsTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_no_data",
      "lineno": 155,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_in_draft_no_data",
        "TestGetStatsTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_in_draft.py::TestGetStatsTimeInDraft::test_get_stats_time_in_draft_with_data",
      "lineno": 134,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_in_draft_with_data",
        "TestGetStatsTimeInDraft",
        "test_time_in_draft.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Average time in draft: 2 days, 0:00:00\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_none_for_empty_list",
      "lineno": 18,
      "outcome": "passed",
      "keywords": [
        "test_returns_none_for_empty_list",
        "TestGetAverageTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_none_for_list_with_no_time_to_answer",
      "lineno": 29,
      "outcome": "passed",
      "keywords": [
        "test_returns_none_for_list_with_no_time_to_answer",
        "TestGetAverageTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestGetAverageTimeToAnswer::test_returns_stats_time_to_answer",
      "lineno": 46,
      "outcome": "passed",
      "keywords": [
        "test_returns_stats_time_to_answer",
        "TestGetAverageTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Average time to answer: 0:00:20\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_none_if_answer_chosen_at_is_missing",
      "lineno": 81,
      "outcome": "passed",
      "keywords": [
        "test_returns_none_if_answer_chosen_at_is_missing",
        "TestMeasureTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_none_if_created_at_is_missing",
      "lineno": 95,
      "outcome": "passed",
      "keywords": [
        "test_returns_none_if_created_at_is_missing",
        "TestMeasureTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_answer.py::TestMeasureTimeToAnswer::test_returns_time_to_answer",
      "lineno": 109,
      "outcome": "passed",
      "keywords": [
        "test_returns_time_to_answer",
        "TestMeasureTimeToAnswer",
        "test_time_to_answer.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose::test_get_stats_time_to_close",
      "lineno": 23,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_to_close",
        "TestGetAverageTimeToClose",
        "test_time_to_close.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Time to close: 3 days, 0:00:00\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_close.py::TestGetAverageTimeToClose::test_get_stats_time_to_close_no_issues",
      "lineno": 51,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_to_close_no_issues",
        "TestGetAverageTimeToClose",
        "test_time_to_close.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close",
      "lineno": 75,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_close",
        "TestMeasureTimeToClose",
        "test_time_to_close.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close_discussion",
      "lineno": 97,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_close_discussion",
        "TestMeasureTimeToClose",
        "test_time_to_close.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_close.py::TestMeasureTimeToClose::test_measure_time_to_close_returns_none",
      "lineno": 88,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_close_returns_none",
        "TestMeasureTimeToClose",
        "test_time_to_close.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response",
      "lineno": 27,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_bot",
      "lineno": 341,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_ignore_bot",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_issue_owners_comment",
      "lineno": 305,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_ignore_issue_owners_comment",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_pending_review",
      "lineno": 235,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_ignore_pending_review",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_ignore_users",
      "lineno": 197,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_ignore_users",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_issue_comment_faster",
      "lineno": 97,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_issue_comment_faster",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_no_comments",
      "lineno": 54,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_no_comments",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_only_ignored_users",
      "lineno": 265,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_only_ignored_users",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_pull_request_comment_faster",
      "lineno": 128,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_pull_request_comment_faster",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_pull_request_comment_ignore_before_ready",
      "lineno": 159,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_pull_request_comment_ignore_before_ready",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestMeasureTimeToFirstResponse::test_measure_time_to_first_response_with_pull_request_comments",
      "lineno": 69,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_first_response_with_pull_request_comments",
        "TestMeasureTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse::test_get_stats_time_to_first_response",
      "lineno": 379,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_to_first_response",
        "TestGetStatsTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed",
        "stdout": "Average time to first response: 1 day, 12:00:00\n"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_first_response.py::TestGetStatsTimeToFirstResponse::test_get_stats_time_to_first_response_with_all_none",
      "lineno": 411,
      "outcome": "passed",
      "keywords": [
        "test_get_stats_time_to_first_response_with_all_none",
        "TestGetStatsTimeToFirstResponse",
        "test_time_to_first_response.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_created_at",
      "lineno": 33,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_merge_created_at",
        "TestMeasureTimeToMerge",
        "test_time_to_merge.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_ready_for_review",
      "lineno": 21,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_merge_ready_for_review",
        "TestMeasureTimeToMerge",
        "test_time_to_merge.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_merge.py::TestMeasureTimeToMerge::test_measure_time_to_merge_returns_none",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_measure_time_to_merge_returns_none",
        "TestMeasureTimeToMerge",
        "test_time_to_merge.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_get_time_to_ready_for_review_event",
      "lineno": 32,
      "outcome": "passed",
      "keywords": [
        "test_get_time_to_ready_for_review_event",
        "TestGetTimeToReadyForReview",
        "test_time_to_ready_for_review.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_get_time_to_ready_for_review_no_event",
      "lineno": 46,
      "outcome": "passed",
      "keywords": [
        "test_get_time_to_ready_for_review_no_event",
        "TestGetTimeToReadyForReview",
        "test_time_to_ready_for_review.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "test_time_to_ready_for_review.py::TestGetTimeToReadyForReview::test_time_to_ready_for_review_draft",
      "lineno": 22,
      "outcome": "passed",
      "keywords": [
        "test_time_to_ready_for_review_draft",
        "TestGetTimeToReadyForReview",
        "test_time_to_ready_for_review.py",
        "github@issue-metrics__44877577__python-dotenv__dynaconf",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    }
  ],
  "warnings": [
    {
      "message": "Usage of `from dynaconf import settings` is now DEPRECATED in 3.0.0+. You are encouraged to change it to your own instance e.g: `settings = Dynaconf(*options)`",
      "category": "DeprecationWarning",
      "when": "runtest",
      "filename": "D:\\repos\\github@issue-metrics__44877577__python-dotenv__dynaconf\\.venv\\Lib\\site-packages\\dynaconf\\base.py",
      "lineno": 170
    }
  ]
}