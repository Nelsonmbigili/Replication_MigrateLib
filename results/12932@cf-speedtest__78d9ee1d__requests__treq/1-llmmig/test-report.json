{
  "exitcode": 1,
  "summary": {
    "failed": 13,
    "skipped": 1,
    "error": 5,
    "passed": 32,
    "total": 51,
    "collected": 51
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": ".",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "cf_speedtest",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "cf_speedtest.egg-info",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/integration_test.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/integration_test.py::test_country",
          "type": "Function",
          "lineno": 10
        },
        {
          "nodeid": "tests/integration_test.py::test_preamble",
          "type": "Function",
          "lineno": 17
        },
        {
          "nodeid": "tests/integration_test.py::test_main",
          "type": "Function",
          "lineno": 25
        },
        {
          "nodeid": "tests/integration_test.py::test_proxy",
          "type": "Function",
          "lineno": 30
        },
        {
          "nodeid": "tests/integration_test.py::test_nossl",
          "type": "Function",
          "lineno": 36
        },
        {
          "nodeid": "tests/integration_test.py::test_csv_output",
          "type": "Function",
          "lineno": 41
        }
      ]
    },
    {
      "nodeid": "tests/network_test.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/network_test.py::test_get_our_country",
          "type": "Function",
          "lineno": 16
        },
        {
          "nodeid": "tests/network_test.py::test_preamble_unit",
          "type": "Function",
          "lineno": 24
        }
      ]
    },
    {
      "nodeid": "tests/options_test.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[yes-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[no-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[true-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[false-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[1-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[0-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[YES-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[NO-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[True-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[False-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[y-True]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_valid[n-False]",
          "type": "Function",
          "lineno": 8
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_invalid[invalid]",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_invalid[maybe]",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_invalid[2]",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/options_test.py::test_str_to_bool_invalid[-1]",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_valid[0-0]",
          "type": "Function",
          "lineno": 34
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_valid[50-50]",
          "type": "Function",
          "lineno": 34
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_valid[100-100]",
          "type": "Function",
          "lineno": 34
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_invalid[-1]",
          "type": "Function",
          "lineno": 45
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_invalid[101]",
          "type": "Function",
          "lineno": 45
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_invalid[invalid]",
          "type": "Function",
          "lineno": 45
        },
        {
          "nodeid": "tests/options_test.py::test_valid_percentile_invalid[50.5]",
          "type": "Function",
          "lineno": 45
        }
      ]
    },
    {
      "nodeid": "tests/speedtest_test.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/speedtest_test.py::test_run_tests[down-1000-3-3]",
          "type": "Function",
          "lineno": 10
        },
        {
          "nodeid": "tests/speedtest_test.py::test_run_tests[up-1000-5-5]",
          "type": "Function",
          "lineno": 10
        },
        {
          "nodeid": "tests/speedtest_test.py::test_run_tests[invalid-1000-2-0]",
          "type": "Function",
          "lineno": 10
        },
        {
          "nodeid": "tests/speedtest_test.py::test_run_standard_test",
          "type": "Function",
          "lineno": 32
        },
        {
          "nodeid": "tests/speedtest_test.py::test_main_unit[args0-0]",
          "type": "Function",
          "lineno": 51
        },
        {
          "nodeid": "tests/speedtest_test.py::test_main_unit[args1-0]",
          "type": "Function",
          "lineno": 51
        },
        {
          "nodeid": "tests/speedtest_test.py::test_main_unit[args2-0]",
          "type": "Function",
          "lineno": 51
        },
        {
          "nodeid": "tests/speedtest_test.py::test_proxy_unit[100.24.216.83:80-expected_dict0]",
          "type": "Function",
          "lineno": 73
        },
        {
          "nodeid": "tests/speedtest_test.py::test_proxy_unit[socks5://127.0.0.1:9150-expected_dict1]",
          "type": "Function",
          "lineno": 73
        },
        {
          "nodeid": "tests/speedtest_test.py::test_proxy_unit[http://user:pass@10.10.1.10:3128-expected_dict2]",
          "type": "Function",
          "lineno": 73
        },
        {
          "nodeid": "tests/speedtest_test.py::test_output_file",
          "type": "Function",
          "lineno": 109
        }
      ]
    },
    {
      "nodeid": "tests/utils_test.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/utils_test.py::test_percentile[data0-50-3]",
          "type": "Function",
          "lineno": 7
        },
        {
          "nodeid": "tests/utils_test.py::test_percentile[data1-90-9]",
          "type": "Function",
          "lineno": 7
        },
        {
          "nodeid": "tests/utils_test.py::test_percentile[data2-100-1]",
          "type": "Function",
          "lineno": 7
        },
        {
          "nodeid": "tests/utils_test.py::test_percentile[data3-0-1]",
          "type": "Function",
          "lineno": 7
        },
        {
          "nodeid": "tests/utils_test.py::test_get_server_timing[dur=1234.5-1.2345]",
          "type": "Function",
          "lineno": 19
        },
        {
          "nodeid": "tests/utils_test.py::test_get_server_timing[key=value;dur=5678.9-5.6789]",
          "type": "Function",
          "lineno": 19
        },
        {
          "nodeid": "tests/utils_test.py::test_get_server_timing[invalid-0.0]",
          "type": "Function",
          "lineno": 19
        },
        {
          "nodeid": "tests/utils_test.py::test_get_server_timing[dur=1000-1.0]",
          "type": "Function",
          "lineno": 19
        },
        {
          "nodeid": "tests/utils_test.py::test_get_server_timing[start=0;dur=500;desc=\"Backend\"-0.5]",
          "type": "Function",
          "lineno": 19
        }
      ]
    },
    {
      "nodeid": "tests",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/integration_test.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/network_test.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/options_test.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/speedtest_test.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/utils_test.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": ".",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "cf_speedtest",
          "type": "Package"
        },
        {
          "nodeid": "cf_speedtest.egg-info",
          "type": "Dir"
        },
        {
          "nodeid": "tests",
          "type": "Package"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/integration_test.py::test_country",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_country",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
          "lineno": 14,
          "message": "assert False\n +  where False = isinstance(<coroutine object get_our_country at 0x00000229CDD21A40>, str)"
        },
        "traceback": [
          {
            "path": "tests\\integration_test.py",
            "lineno": 14,
            "message": "AssertionError"
          }
        ],
        "longrepr": "@pytest.mark.integration\n    def test_country():\n        country = speedtest.get_our_country()\n>       assert isinstance(country, str)\nE       assert False\nE        +  where False = isinstance(<coroutine object get_our_country at 0x00000229CDD21A40>, str)\n\ntests\\integration_test.py:14: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/integration_test.py::test_preamble",
      "lineno": 17,
      "outcome": "failed",
      "keywords": [
        "test_preamble",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
          "lineno": 21,
          "message": "assert False\n +  where False = isinstance(<coroutine object preamble at 0x00000229CDC2A9D0>, str)"
        },
        "traceback": [
          {
            "path": "tests\\integration_test.py",
            "lineno": 21,
            "message": "AssertionError"
          }
        ],
        "longrepr": "@pytest.mark.integration\n    def test_preamble():\n        preamble_text = speedtest.preamble()\n>       assert isinstance(preamble_text, str)\nE       assert False\nE        +  where False = isinstance(<coroutine object preamble at 0x00000229CDC2A9D0>, str)\n\ntests\\integration_test.py:21: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/integration_test.py::test_main",
      "lineno": 25,
      "outcome": "failed",
      "keywords": [
        "test_main",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
          "lineno": 28,
          "message": "assert <coroutine object main at 0x00000229CDC99000> == 0\n +  where <coroutine object main at 0x00000229CDC99000> = <function main at 0x00000229CDCC53A0>()\n +    where <function main at 0x00000229CDCC53A0> = speedtest.main"
        },
        "traceback": [
          {
            "path": "tests\\integration_test.py",
            "lineno": 28,
            "message": "AssertionError"
          }
        ],
        "longrepr": "@pytest.mark.integration\n    def test_main():\n>       assert speedtest.main() == 0\nE       assert <coroutine object main at 0x00000229CDC99000> == 0\nE        +  where <coroutine object main at 0x00000229CDC99000> = <function main at 0x00000229CDCC53A0>()\nE        +    where <function main at 0x00000229CDCC53A0> = speedtest.main\n\ntests\\integration_test.py:28: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/integration_test.py::test_proxy",
      "lineno": 30,
      "outcome": "skipped",
      "keywords": [
        "test_proxy",
        "skip",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "skipped",
        "longrepr": "('D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\tests\\\\integration_test.py', 31, 'Skipped: will fail without proxy')"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/integration_test.py::test_nossl",
      "lineno": 36,
      "outcome": "failed",
      "keywords": [
        "test_nossl",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
          "lineno": 39,
          "message": "AssertionError: assert <coroutine object main at 0x00000229CDC992A0> == 0\n +  where <coroutine object main at 0x00000229CDC992A0> = <function main at 0x00000229CDCC53A0>(['--verifyssl', 'False'])\n +    where <function main at 0x00000229CDCC53A0> = speedtest.main"
        },
        "traceback": [
          {
            "path": "tests\\integration_test.py",
            "lineno": 39,
            "message": "AssertionError"
          }
        ],
        "longrepr": "@pytest.mark.integration\n    def test_nossl():\n>       assert speedtest.main(['--verifyssl', 'False']) == 0\nE       AssertionError: assert <coroutine object main at 0x00000229CDC992A0> == 0\nE        +  where <coroutine object main at 0x00000229CDC992A0> = <function main at 0x00000229CDCC53A0>(['--verifyssl', 'False'])\nE        +    where <function main at 0x00000229CDCC53A0> = speedtest.main\n\ntests\\integration_test.py:39: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/integration_test.py::test_csv_output",
      "lineno": 41,
      "outcome": "failed",
      "keywords": [
        "test_csv_output",
        "integration",
        "pytestmark",
        "integration_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
          "lineno": 46,
          "message": "AssertionError: assert <coroutine object main at 0x00000229CDC99540> == 0\n +  where <coroutine object main at 0x00000229CDC99540> = <function main at 0x00000229CDCC53A0>(['--output', 'test_output.csv'])\n +    where <function main at 0x00000229CDCC53A0> = speedtest.main"
        },
        "traceback": [
          {
            "path": "tests\\integration_test.py",
            "lineno": 46,
            "message": "AssertionError"
          }
        ],
        "longrepr": "@pytest.mark.integration\n    def test_csv_output():\n        temp_file = 'test_output.csv'\n    \n>       assert speedtest.main(['--output', temp_file]) == 0\nE       AssertionError: assert <coroutine object main at 0x00000229CDC99540> == 0\nE        +  where <coroutine object main at 0x00000229CDC99540> = <function main at 0x00000229CDCC53A0>(['--output', 'test_output.csv'])\nE        +    where <function main at 0x00000229CDCC53A0> = speedtest.main\n\ntests\\integration_test.py:46: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/network_test.py::test_get_our_country",
      "lineno": 16,
      "outcome": "error",
      "keywords": [
        "test_get_our_country",
        "network_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'"
        },
        "traceback": [
          {
            "path": "tests\\network_test.py",
            "lineno": 13,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "@pytest.fixture\n    def mock_requests_session():\n>       with patch('cf_speedtest.speedtest.REQ_SESSION') as mock_session:\n\ntests\\network_test.py:13: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000229CDDA5C40>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/network_test.py::test_preamble_unit",
      "lineno": 24,
      "outcome": "error",
      "keywords": [
        "test_preamble_unit",
        "network_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'"
        },
        "traceback": [
          {
            "path": "tests\\network_test.py",
            "lineno": 13,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "@pytest.fixture\n    def mock_requests_session():\n>       with patch('cf_speedtest.speedtest.REQ_SESSION') as mock_session:\n\ntests\\network_test.py:13: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000229CDDA6DB0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[yes-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[yes-True]",
        "parametrize",
        "pytestmark",
        "yes-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[no-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[no-False]",
        "parametrize",
        "pytestmark",
        "no-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[true-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[true-True]",
        "parametrize",
        "pytestmark",
        "true-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[false-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[false-False]",
        "parametrize",
        "pytestmark",
        "false-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[1-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[1-True]",
        "parametrize",
        "pytestmark",
        "1-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[0-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[0-False]",
        "parametrize",
        "pytestmark",
        "0-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[YES-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[YES-True]",
        "parametrize",
        "pytestmark",
        "YES-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[NO-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[NO-False]",
        "parametrize",
        "pytestmark",
        "NO-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[True-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[True-True]",
        "parametrize",
        "pytestmark",
        "True-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[False-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[False-False]",
        "parametrize",
        "pytestmark",
        "False-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[y-True]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[y-True]",
        "parametrize",
        "pytestmark",
        "y-True",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_valid[n-False]",
      "lineno": 8,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_valid[n-False]",
        "parametrize",
        "pytestmark",
        "n-False",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_invalid[invalid]",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_invalid[invalid]",
        "parametrize",
        "pytestmark",
        "invalid",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_invalid[maybe]",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_invalid[maybe]",
        "parametrize",
        "pytestmark",
        "maybe",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_invalid[2]",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_invalid[2]",
        "parametrize",
        "pytestmark",
        "2",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_str_to_bool_invalid[-1]",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_str_to_bool_invalid[-1]",
        "parametrize",
        "pytestmark",
        "-1",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_valid[0-0]",
      "lineno": 34,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_valid[0-0]",
        "parametrize",
        "pytestmark",
        "0-0",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_valid[50-50]",
      "lineno": 34,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_valid[50-50]",
        "parametrize",
        "pytestmark",
        "50-50",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_valid[100-100]",
      "lineno": 34,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_valid[100-100]",
        "parametrize",
        "pytestmark",
        "100-100",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_invalid[-1]",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_invalid[-1]",
        "parametrize",
        "pytestmark",
        "-1",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_invalid[101]",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_invalid[101]",
        "parametrize",
        "pytestmark",
        "101",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_invalid[invalid]",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_invalid[invalid]",
        "parametrize",
        "pytestmark",
        "invalid",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/options_test.py::test_valid_percentile_invalid[50.5]",
      "lineno": 45,
      "outcome": "passed",
      "keywords": [
        "test_valid_percentile_invalid[50.5]",
        "parametrize",
        "pytestmark",
        "50.5",
        "options_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_run_tests[down-1000-3-3]",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_run_tests[down-1000-3-3]",
        "parametrize",
        "pytestmark",
        "down-1000-3-3",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 25,
          "message": "TypeError: object of type 'coroutine' has no len()"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 25,
            "message": "TypeError"
          }
        ],
        "longrepr": "test_type = 'down', bytes_to_xfer = 1000, iteration_count = 3, expected_len = 3\n\n    @pytest.mark.parametrize(\n        'test_type, bytes_to_xfer, iteration_count, expected_len', [\n            ('down', 1000, 3, 3),\n            ('up', 1000, 5, 5),\n            ('invalid', 1000, 2, 0),\n        ],\n    )\n    def test_run_tests(test_type, bytes_to_xfer, iteration_count, expected_len):\n        with patch('cf_speedtest.speedtest.download_test', return_value=(1000, 0.1)), \\\n                patch('cf_speedtest.speedtest.upload_test', return_value=(1000, 0.2)):\n    \n            results = speedtest.run_tests(\n                test_type, bytes_to_xfer, iteration_count,\n            )\n>           assert len(results) == expected_len\nE           TypeError: object of type 'coroutine' has no len()\n\ntests\\speedtest_test.py:25: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_run_tests[up-1000-5-5]",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_run_tests[up-1000-5-5]",
        "parametrize",
        "pytestmark",
        "up-1000-5-5",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 25,
          "message": "TypeError: object of type 'coroutine' has no len()"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 25,
            "message": "TypeError"
          }
        ],
        "longrepr": "test_type = 'up', bytes_to_xfer = 1000, iteration_count = 5, expected_len = 5\n\n    @pytest.mark.parametrize(\n        'test_type, bytes_to_xfer, iteration_count, expected_len', [\n            ('down', 1000, 3, 3),\n            ('up', 1000, 5, 5),\n            ('invalid', 1000, 2, 0),\n        ],\n    )\n    def test_run_tests(test_type, bytes_to_xfer, iteration_count, expected_len):\n        with patch('cf_speedtest.speedtest.download_test', return_value=(1000, 0.1)), \\\n                patch('cf_speedtest.speedtest.upload_test', return_value=(1000, 0.2)):\n    \n            results = speedtest.run_tests(\n                test_type, bytes_to_xfer, iteration_count,\n            )\n>           assert len(results) == expected_len\nE           TypeError: object of type 'coroutine' has no len()\n\ntests\\speedtest_test.py:25: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_run_tests[invalid-1000-2-0]",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_run_tests[invalid-1000-2-0]",
        "parametrize",
        "pytestmark",
        "invalid-1000-2-0",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 25,
          "message": "TypeError: object of type 'coroutine' has no len()"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 25,
            "message": "TypeError"
          }
        ],
        "longrepr": "test_type = 'invalid', bytes_to_xfer = 1000, iteration_count = 2\nexpected_len = 0\n\n    @pytest.mark.parametrize(\n        'test_type, bytes_to_xfer, iteration_count, expected_len', [\n            ('down', 1000, 3, 3),\n            ('up', 1000, 5, 5),\n            ('invalid', 1000, 2, 0),\n        ],\n    )\n    def test_run_tests(test_type, bytes_to_xfer, iteration_count, expected_len):\n        with patch('cf_speedtest.speedtest.download_test', return_value=(1000, 0.1)), \\\n                patch('cf_speedtest.speedtest.upload_test', return_value=(1000, 0.2)):\n    \n            results = speedtest.run_tests(\n                test_type, bytes_to_xfer, iteration_count,\n            )\n>           assert len(results) == expected_len\nE           TypeError: object of type 'coroutine' has no len()\n\ntests\\speedtest_test.py:25: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_run_standard_test",
      "lineno": 32,
      "outcome": "failed",
      "keywords": [
        "test_run_standard_test",
        "__wrapped__",
        "patchings",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 47,
          "message": "TypeError: 'coroutine' object is not subscriptable"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 47,
            "message": "TypeError"
          }
        ],
        "longrepr": "mock_preamble = <AsyncMock name='preamble' id='2378570922128'>\nmock_latency_test = <AsyncMock name='latency_test' id='2378571140800'>\nmock_run_tests = <AsyncMock name='run_tests' id='2378571145984'>\n\n    @patch('cf_speedtest.speedtest.run_tests')\n    @patch('cf_speedtest.speedtest.latency_test')\n    @patch('cf_speedtest.speedtest.preamble')\n    def test_run_standard_test(mock_preamble, mock_latency_test, mock_run_tests):\n        mock_latency_test.return_value = 0.05\n        mock_run_tests.side_effect = [\n            [100000000, 200000000],  # download\n            [50000000, 100000000],  # upload\n        ]\n    \n        results = speedtest.run_standard_test(\n            [1000000], measurement_percentile=90, verbose=True,\n        )\n    \n>       assert results['download_speed'] == 200000000\nE       TypeError: 'coroutine' object is not subscriptable\n\ntests\\speedtest_test.py:47: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_main_unit[args0-0]",
      "lineno": 51,
      "outcome": "error",
      "keywords": [
        "test_main_unit[args0-0]",
        "parametrize",
        "pytestmark",
        "args0-0",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'"
        },
        "traceback": [
          {
            "path": "tests\\conftest.py",
            "lineno": 17,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "@pytest.fixture\n    def mock_requests_session():\n>       with patch('cf_speedtest.speedtest.REQ_SESSION') as mock_session:\n\ntests\\conftest.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000229CDE35A00>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_main_unit[args1-0]",
      "lineno": 51,
      "outcome": "error",
      "keywords": [
        "test_main_unit[args1-0]",
        "parametrize",
        "pytestmark",
        "args1-0",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'"
        },
        "traceback": [
          {
            "path": "tests\\conftest.py",
            "lineno": 17,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "@pytest.fixture\n    def mock_requests_session():\n>       with patch('cf_speedtest.speedtest.REQ_SESSION') as mock_session:\n\ntests\\conftest.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000229CDE34AA0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_main_unit[args2-0]",
      "lineno": 51,
      "outcome": "error",
      "keywords": [
        "test_main_unit[args2-0]",
        "parametrize",
        "pytestmark",
        "args2-0",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'"
        },
        "traceback": [
          {
            "path": "tests\\conftest.py",
            "lineno": 17,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "@pytest.fixture\n    def mock_requests_session():\n>       with patch('cf_speedtest.speedtest.REQ_SESSION') as mock_session:\n\ntests\\conftest.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000229CDDA7470>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'cf_speedtest.speedtest' from 'D:\\\\repos\\\\12932@cf-speedtest__78d9ee1d__requests__treq\\\\cf_speedtest\\\\speedtest.py'> does not have the attribute 'REQ_SESSION'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_proxy_unit[100.24.216.83:80-expected_dict0]",
      "lineno": 73,
      "outcome": "failed",
      "keywords": [
        "test_proxy_unit[100.24.216.83:80-expected_dict0]",
        "parametrize",
        "pytestmark",
        "100.24.216.83:80-expected_dict0",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 107,
          "message": "AssertionError: assert None == {'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80'}\n +  where None = speedtest.PROXY_DICT"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 107,
            "message": "AssertionError"
          }
        ],
        "longrepr": "proxy = '100.24.216.83:80'\nexpected_dict = {'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80'}\n\n    @pytest.mark.parametrize(\n        'proxy, expected_dict', [\n            (\n                '100.24.216.83:80', {\n                    'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80',\n                },\n            ),\n            (\n                'socks5://127.0.0.1:9150',\n                {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'},\n            ),\n            (\n                'http://user:pass@10.10.1.10:3128',\n                {\n                    'http': 'http://user:pass@10.10.1.10:3128',\n                    'https': 'http://user:pass@10.10.1.10:3128',\n                },\n            ),\n        ],\n    )\n    def test_proxy_unit(proxy, expected_dict):\n        with patch('cf_speedtest.speedtest.run_standard_test') as mock_run_test:\n            mock_run_test.return_value = {\n                'download_speed': 100000000,\n                'upload_speed': 50000000,\n                'download_stdev': 1000000,\n                'upload_stdev': 500000,\n                'latency_measurements': [10, 20, 30],\n                'download_measurements': [90000000, 100000000, 110000000],\n                'upload_measurements': [45000000, 50000000, 55000000],\n            }\n    \n            speedtest.main(['--proxy', proxy])\n>           assert speedtest.PROXY_DICT == expected_dict\nE           AssertionError: assert None == {'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80'}\nE            +  where None = speedtest.PROXY_DICT\n\ntests\\speedtest_test.py:107: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_proxy_unit[socks5://127.0.0.1:9150-expected_dict1]",
      "lineno": 73,
      "outcome": "failed",
      "keywords": [
        "test_proxy_unit[socks5://127.0.0.1:9150-expected_dict1]",
        "parametrize",
        "pytestmark",
        "socks5://127.0.0.1:9150-expected_dict1",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 107,
          "message": "AssertionError: assert None == {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'}\n +  where None = speedtest.PROXY_DICT"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 107,
            "message": "AssertionError"
          }
        ],
        "longrepr": "proxy = 'socks5://127.0.0.1:9150'\nexpected_dict = {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'}\n\n    @pytest.mark.parametrize(\n        'proxy, expected_dict', [\n            (\n                '100.24.216.83:80', {\n                    'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80',\n                },\n            ),\n            (\n                'socks5://127.0.0.1:9150',\n                {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'},\n            ),\n            (\n                'http://user:pass@10.10.1.10:3128',\n                {\n                    'http': 'http://user:pass@10.10.1.10:3128',\n                    'https': 'http://user:pass@10.10.1.10:3128',\n                },\n            ),\n        ],\n    )\n    def test_proxy_unit(proxy, expected_dict):\n        with patch('cf_speedtest.speedtest.run_standard_test') as mock_run_test:\n            mock_run_test.return_value = {\n                'download_speed': 100000000,\n                'upload_speed': 50000000,\n                'download_stdev': 1000000,\n                'upload_stdev': 500000,\n                'latency_measurements': [10, 20, 30],\n                'download_measurements': [90000000, 100000000, 110000000],\n                'upload_measurements': [45000000, 50000000, 55000000],\n            }\n    \n            speedtest.main(['--proxy', proxy])\n>           assert speedtest.PROXY_DICT == expected_dict\nE           AssertionError: assert None == {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'}\nE            +  where None = speedtest.PROXY_DICT\n\ntests\\speedtest_test.py:107: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_proxy_unit[http://user:pass@10.10.1.10:3128-expected_dict2]",
      "lineno": 73,
      "outcome": "failed",
      "keywords": [
        "test_proxy_unit[http://user:pass@10.10.1.10:3128-expected_dict2]",
        "parametrize",
        "pytestmark",
        "http://user:pass@10.10.1.10:3128-expected_dict2",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
          "lineno": 107,
          "message": "AssertionError: assert None == {'http': 'http://user:pass@10.10.1.10:3128', 'https': 'http://user:pass@10.10.1.10:3128'}\n +  where None = speedtest.PROXY_DICT"
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 107,
            "message": "AssertionError"
          }
        ],
        "longrepr": "proxy = 'http://user:pass@10.10.1.10:3128'\nexpected_dict = {'http': 'http://user:pass@10.10.1.10:3128', 'https': 'http://user:pass@10.10.1.10:3128'}\n\n    @pytest.mark.parametrize(\n        'proxy, expected_dict', [\n            (\n                '100.24.216.83:80', {\n                    'http': 'http://100.24.216.83:80', 'https': 'http://100.24.216.83:80',\n                },\n            ),\n            (\n                'socks5://127.0.0.1:9150',\n                {'http': 'socks5://127.0.0.1:9150', 'https': 'socks5://127.0.0.1:9150'},\n            ),\n            (\n                'http://user:pass@10.10.1.10:3128',\n                {\n                    'http': 'http://user:pass@10.10.1.10:3128',\n                    'https': 'http://user:pass@10.10.1.10:3128',\n                },\n            ),\n        ],\n    )\n    def test_proxy_unit(proxy, expected_dict):\n        with patch('cf_speedtest.speedtest.run_standard_test') as mock_run_test:\n            mock_run_test.return_value = {\n                'download_speed': 100000000,\n                'upload_speed': 50000000,\n                'download_stdev': 1000000,\n                'upload_stdev': 500000,\n                'latency_measurements': [10, 20, 30],\n                'download_measurements': [90000000, 100000000, 110000000],\n                'upload_measurements': [45000000, 50000000, 55000000],\n            }\n    \n            speedtest.main(['--proxy', proxy])\n>           assert speedtest.PROXY_DICT == expected_dict\nE           AssertionError: assert None == {'http': 'http://user:pass@10.10.1.10:3128', 'https': 'http://user:pass@10.10.1.10:3128'}\nE            +  where None = speedtest.PROXY_DICT\n\ntests\\speedtest_test.py:107: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/speedtest_test.py::test_output_file",
      "lineno": 109,
      "outcome": "failed",
      "keywords": [
        "test_output_file",
        "speedtest_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 935,
          "message": "AssertionError: expected call not found.\nExpected: open('test_output.csv', 'w')\n  Actual: not called."
        },
        "traceback": [
          {
            "path": "tests\\speedtest_test.py",
            "lineno": 127,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 935,
            "message": "AssertionError"
          }
        ],
        "longrepr": "mock_time = <MagicMock name='time' id='2378571631072'>\n\n    def test_output_file(mock_time):\n        output_file = 'test_output.csv'\n    \n        with patch('cf_speedtest.speedtest.run_standard_test') as mock_run_test, \\\n                patch('builtins.open', create=True) as mock_open:\n            mock_run_test.return_value = {\n                'download_speed': 100000000,\n                'upload_speed': 50000000,\n                'download_stdev': 1000000,\n                'upload_stdev': 500000,\n                'latency_measurements': [10, 20, 30],\n                'download_measurements': [90000000, 100000000, 110000000],\n                'upload_measurements': [45000000, 50000000, 55000000],\n            }\n    \n            speedtest.main(['--output', output_file])\n    \n>           mock_open.assert_called_with(output_file, 'w')\n\ntests\\speedtest_test.py:127: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <MagicMock name='open' id='2378571644752'>\nargs = ('test_output.csv', 'w'), kwargs = {}\nexpected = \"open('test_output.csv', 'w')\", actual = 'not called.'\nerror_message = \"expected call not found.\\nExpected: open('test_output.csv', 'w')\\n  Actual: not called.\"\n\n    def assert_called_with(self, /, *args, **kwargs):\n        \"\"\"assert that the last call was made with the specified arguments.\n    \n        Raises an AssertionError if the args and keyword args passed in are\n        different to the last call to the mock.\"\"\"\n        if self.call_args is None:\n            expected = self._format_mock_call_signature(args, kwargs)\n            actual = 'not called.'\n            error_message = ('expected call not found.\\nExpected: %s\\n  Actual: %s'\n                    % (expected, actual))\n>           raise AssertionError(error_message)\nE           AssertionError: expected call not found.\nE           Expected: open('test_output.csv', 'w')\nE             Actual: not called.\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:935: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_percentile[data0-50-3]",
      "lineno": 7,
      "outcome": "passed",
      "keywords": [
        "test_percentile[data0-50-3]",
        "parametrize",
        "pytestmark",
        "data0-50-3",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_percentile[data1-90-9]",
      "lineno": 7,
      "outcome": "passed",
      "keywords": [
        "test_percentile[data1-90-9]",
        "parametrize",
        "pytestmark",
        "data1-90-9",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_percentile[data2-100-1]",
      "lineno": 7,
      "outcome": "passed",
      "keywords": [
        "test_percentile[data2-100-1]",
        "parametrize",
        "pytestmark",
        "data2-100-1",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_percentile[data3-0-1]",
      "lineno": 7,
      "outcome": "passed",
      "keywords": [
        "test_percentile[data3-0-1]",
        "parametrize",
        "pytestmark",
        "data3-0-1",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_get_server_timing[dur=1234.5-1.2345]",
      "lineno": 19,
      "outcome": "passed",
      "keywords": [
        "test_get_server_timing[dur=1234.5-1.2345]",
        "parametrize",
        "pytestmark",
        "dur=1234.5-1.2345",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_get_server_timing[key=value;dur=5678.9-5.6789]",
      "lineno": 19,
      "outcome": "passed",
      "keywords": [
        "test_get_server_timing[key=value;dur=5678.9-5.6789]",
        "parametrize",
        "pytestmark",
        "key=value;dur=5678.9-5.6789",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_get_server_timing[invalid-0.0]",
      "lineno": 19,
      "outcome": "passed",
      "keywords": [
        "test_get_server_timing[invalid-0.0]",
        "parametrize",
        "pytestmark",
        "invalid-0.0",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_get_server_timing[dur=1000-1.0]",
      "lineno": 19,
      "outcome": "passed",
      "keywords": [
        "test_get_server_timing[dur=1000-1.0]",
        "parametrize",
        "pytestmark",
        "dur=1000-1.0",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/utils_test.py::test_get_server_timing[start=0;dur=500;desc=\"Backend\"-0.5]",
      "lineno": 19,
      "outcome": "passed",
      "keywords": [
        "test_get_server_timing[start=0;dur=500;desc=\"Backend\"-0.5]",
        "parametrize",
        "pytestmark",
        "start=0;dur=500;desc=\"Backend\"-0.5",
        "utils_test.py",
        "tests",
        "12932@cf-speedtest__78d9ee1d__requests__treq",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    }
  ],
  "warnings": [
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 11
    },
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 18
    },
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 26
    },
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 31
    },
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 37
    },
    {
      "message": "Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html",
      "category": "PytestUnknownMarkWarning",
      "when": "collect",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\integration_test.py",
      "lineno": 42
    },
    {
      "message": "coroutine 'main' was never awaited",
      "category": "RuntimeWarning",
      "when": "runtest",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
      "lineno": 106
    },
    {
      "message": "coroutine 'main' was never awaited",
      "category": "RuntimeWarning",
      "when": "runtest",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
      "lineno": 106
    },
    {
      "message": "coroutine 'main' was never awaited",
      "category": "RuntimeWarning",
      "when": "runtest",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
      "lineno": 106
    },
    {
      "message": "coroutine 'main' was never awaited",
      "category": "RuntimeWarning",
      "when": "runtest",
      "filename": "D:\\repos\\12932@cf-speedtest__78d9ee1d__requests__treq\\tests\\speedtest_test.py",
      "lineno": 125
    }
  ]
}