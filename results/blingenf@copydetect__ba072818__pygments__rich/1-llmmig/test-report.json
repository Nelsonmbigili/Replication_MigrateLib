{
  "exitcode": 1,
  "summary": {
    "failed": 19,
    "passed": 19,
    "total": 38,
    "collected": 38
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": ".",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "copydetect/data",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "copydetect/winnow",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "copydetect",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "copydetect/data",
          "type": "Package"
        },
        {
          "nodeid": "copydetect/winnow",
          "type": "Package"
        }
      ]
    },
    {
      "nodeid": "copydetect.egg-info",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "docs/_static",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "docs",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "docs/_static",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "tests/sample_other",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/sample_py/boilerplate",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/sample_py/code",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/sample_py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/sample_py/boilerplate",
          "type": "Dir"
        },
        {
          "nodeid": "tests/sample_py/code",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "tests/sample_sanity_check",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare",
          "type": "Function",
          "lineno": 24
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_manual_config",
          "type": "Function",
          "lineno": 62
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_saving",
          "type": "Function",
          "lineno": 78
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_boilerplate",
          "type": "Function",
          "lineno": 97
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_severalfiles",
          "type": "Function",
          "lineno": 116
        }
      ]
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection::test_compare",
          "type": "Function",
          "lineno": 145
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection::test_compare_boilerplate",
          "type": "Function",
          "lineno": 156
        }
      ]
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_ignore_leaf",
          "type": "Function",
          "lineno": 172
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_same_name_only",
          "type": "Function",
          "lineno": 181
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_disable_filtering",
          "type": "Function",
          "lineno": 189
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_force_language",
          "type": "Function",
          "lineno": 198
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_truncation",
          "type": "Function",
          "lineno": 209
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_out_file",
          "type": "Function",
          "lineno": 219
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters::test_encoding_specification",
          "type": "Function",
          "lineno": 238
        }
      ]
    },
    {
      "nodeid": "tests/test_detector.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_detector.py::TestTwoFileDetection",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_detector.py::TestParameters",
          "type": "Class"
        }
      ]
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_empty",
          "type": "Function",
          "lineno": 18
        },
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_1",
          "type": "Function",
          "lineno": 23
        },
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_2",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_3",
          "type": "Function",
          "lineno": 33
        },
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_inf",
          "type": "Function",
          "lineno": 38
        }
      ]
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowDensity",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowDensity::test_winnow_density",
          "type": "Function",
          "lineno": 47
        }
      ]
    },
    {
      "nodeid": "tests/test_pywinnowing.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_pywinnowing.py::TestWinnowDensity",
          "type": "Class"
        }
      ]
    },
    {
      "nodeid": "tests/test_sanity_checks.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_sanity_checks.py::test_rot_c",
          "type": "Function",
          "lineno": 12
        },
        {
          "nodeid": "tests/test_sanity_checks.py::test_sample_xml",
          "type": "Function",
          "lineno": 23
        }
      ]
    },
    {
      "nodeid": "tests/test_utils.py::TestSmallDoc",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_utils.py::TestSmallDoc::test_doc_overlap",
          "type": "Function",
          "lineno": 18
        },
        {
          "nodeid": "tests/test_utils.py::TestSmallDoc::test_doc_overlap_boilerplate",
          "type": "Function",
          "lineno": 26
        },
        {
          "nodeid": "tests/test_utils.py::TestSmallDoc::test_slice_computation",
          "type": "Function",
          "lineno": 36
        },
        {
          "nodeid": "tests/test_utils.py::TestSmallDoc::test_highlighting",
          "type": "Function",
          "lineno": 47
        }
      ]
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerPythonSample",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_utils.py::TestTokenizerPythonSample::test_tokenization",
          "type": "Function",
          "lineno": 81
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerPythonSample::test_copydetect",
          "type": "Function",
          "lineno": 96
        }
      ]
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_php_tokenization",
          "type": "Function",
          "lineno": 139
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_c_tokenization",
          "type": "Function",
          "lineno": 151
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_java_tokenization",
          "type": "Function",
          "lineno": 162
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_get_token_coverage",
          "type": "Function",
          "lineno": 175
        }
      ]
    },
    {
      "nodeid": "tests/test_utils.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_utils.py::TestSmallDoc",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerPythonSample",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples",
          "type": "Class"
        }
      ]
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_empty",
          "type": "Function",
          "lineno": 28
        },
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_1",
          "type": "Function",
          "lineno": 33
        },
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_2",
          "type": "Function",
          "lineno": 38
        },
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_3",
          "type": "Function",
          "lineno": 43
        },
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_inf",
          "type": "Function",
          "lineno": 48
        }
      ]
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowDensity",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowDensity::test_winnow_density",
          "type": "Function",
          "lineno": 57
        }
      ]
    },
    {
      "nodeid": "tests/test_winnowing.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowOutput",
          "type": "Class"
        },
        {
          "nodeid": "tests/test_winnowing.py::TestWinnowDensity",
          "type": "Class"
        }
      ]
    },
    {
      "nodeid": "tests",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/sample_other",
          "type": "Dir"
        },
        {
          "nodeid": "tests/sample_py",
          "type": "Dir"
        },
        {
          "nodeid": "tests/sample_sanity_check",
          "type": "Dir"
        },
        {
          "nodeid": "tests/test_detector.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_pywinnowing.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_sanity_checks.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_utils.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/test_winnowing.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": ".",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "copydetect",
          "type": "Package"
        },
        {
          "nodeid": "copydetect.egg-info",
          "type": "Dir"
        },
        {
          "nodeid": "docs",
          "type": "Dir"
        },
        {
          "nodeid": "tests",
          "type": "Dir"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare",
      "lineno": 24,
      "outcome": "failed",
      "keywords": [
        "test_compare",
        "TestTwoFileDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 36,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileDetection object at 0x0000020ECF7C3590>\nsample_file_metrics = {'file1_len': 2052, 'file2_len': 1257, 'token_overlap': 1155}\n\n    def test_compare(self, sample_file_metrics):\n        config = {\n            \"test_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"reference_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"extensions\" : [\"py\"],\n            \"noise_threshold\" : 25,\n            \"guarantee_threshold\" : 25,\n            \"display_threshold\" : 0,\n            \"silent\" : True\n        }\n        detector = CopyDetector.from_config(config)\n>       detector.run()\n\ntests\\test_detector.py:36: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_manual_config",
      "lineno": 62,
      "outcome": "failed",
      "keywords": [
        "test_compare_manual_config",
        "TestTwoFileDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 67,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileDetection object at 0x0000020ECF7C36B0>\nsample_file_metrics = {'file1_len': 2052, 'file2_len': 1257, 'token_overlap': 1155}\n\n    def test_compare_manual_config(self, sample_file_metrics):\n        detector = CopyDetector(noise_t=25, guarantee_t=25, silent=True)\n        detector.add_file(TESTS_DIR + \"/sample_py/code/sample1.py\")\n        detector.add_file(TESTS_DIR + \"/sample_py/code/sample2.py\")\n>       detector.run()\n\ntests\\test_detector.py:67: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = \"import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ... for sv in s:\\n              plt.scatter(sv[0], sv[1], s=200, facecolors='none', edgecolors='black')\\n\\n  plt.show()\\n\"\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests/sample_py/code/sample1.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_saving",
      "lineno": 78,
      "outcome": "failed",
      "keywords": [
        "test_compare_saving",
        "TestTwoFileDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 92,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileDetection object at 0x0000020ECF7C3860>\ntmpdir = local('C:\\\\Users\\\\Mohay\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Mohay\\\\pytest-1072\\\\test_compare_saving0')\n\n    def test_compare_saving(self, tmpdir):\n        config = {\n            \"test_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"reference_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"extensions\" : [\"py\"],\n            \"noise_threshold\" : 25,\n            \"guarantee_threshold\" : 25,\n            \"display_threshold\" : 0,\n            \"disable_autoopen\" : True,\n            \"out_file\" : tmpdir,\n            \"silent\" : True\n        }\n        detector = CopyDetector.from_config(config)\n>       detector.run()\n\ntests\\test_detector.py:92: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_compare_boilerplate",
      "lineno": 97,
      "outcome": "failed",
      "keywords": [
        "test_compare_boilerplate",
        "TestTwoFileDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 110,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 375,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 355,
            "message": "in _get_boilerplate_hashes"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileDetection object at 0x0000020ECF7C3950>\n\n    def test_compare_boilerplate(self):\n        config = {\n            \"test_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"reference_directories\" : [TESTS_DIR + \"/sample_py/code\"],\n            \"boilerplate_directories\" : [TESTS_DIR + \"/sample_py/boilerplate\"],\n            \"extensions\" : [\"py\"],\n            \"noise_threshold\" : 25,\n            \"guarantee_threshold\" : 25,\n            \"display_threshold\" : 0,\n            \"silent\": True\n        }\n        detector = CopyDetector.from_config(config)\n>       detector.run()\n\ntests\\test_detector.py:110: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:375: in _preprocess_code\n    boilerplate_hashes = self._get_boilerplate_hashes()\ncopydetect\\detector.py:355: in _get_boilerplate_hashes\n    fingerprint = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ...2])-1] + shapes[int(item[2])-1])\\n  m = max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\boilerplate\\\\handout.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileDetection::test_severalfiles",
      "lineno": 116,
      "outcome": "failed",
      "keywords": [
        "test_severalfiles",
        "TestTwoFileDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 133,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileDetection object at 0x0000020ECF7C39B0>\ntmpdir = local('C:\\\\Users\\\\Mohay\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Mohay\\\\pytest-1072\\\\test_severalfiles0')\n\n    def test_severalfiles(self, tmpdir):\n        \"\"\"Run the detector over all the files in the tests directory\n        and perform some basic sanity checking.\n        \"\"\"\n        config = {\n            \"test_directories\" : [TESTS_DIR],\n            \"reference_directories\" : [TESTS_DIR],\n            \"extensions\" : [\"*\"],\n            \"noise_threshold\" : 25,\n            \"guarantee_threshold\" : 30,\n            \"display_threshold\" : 0.3,\n            \"disable_autoopen\" : True,\n            \"out_file\" : tmpdir,\n            \"silent\": True\n        }\n        detector = CopyDetector.from_config(config)\n>       detector.run()\n\ntests\\test_detector.py:133: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = \"import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ... for sv in s:\\n              plt.scatter(sv[0], sv[1], s=200, facecolors='none', edgecolors='black')\\n\\n  plt.show()\\n\"\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample1.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection::test_compare",
      "lineno": 145,
      "outcome": "failed",
      "keywords": [
        "test_compare",
        "TestTwoFileAPIDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 147,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileAPIDetection object at 0x0000020ECF7C3A40>\nsample_file_metrics = {'file1_len': 2052, 'file2_len': 1257, 'token_overlap': 1155}\n\n    def test_compare(self, sample_file_metrics):\n>       fp1 = CodeFingerprint(TESTS_DIR+\"/sample_py/code/sample1.py\", 25, 1)\n\ntests\\test_detector.py:147: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = \"import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ... for sv in s:\\n              plt.scatter(sv[0], sv[1], s=200, facecolors='none', edgecolors='black')\\n\\n  plt.show()\\n\"\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests/sample_py/code/sample1.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestTwoFileAPIDetection::test_compare_boilerplate",
      "lineno": 156,
      "outcome": "failed",
      "keywords": [
        "test_compare_boilerplate",
        "TestTwoFileAPIDetection",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 158,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestTwoFileAPIDetection object at 0x0000020ECF7C3CB0>\n\n    def test_compare_boilerplate(self):\n>       bp_fingerprint = CodeFingerprint(\n            TESTS_DIR + \"/sample_py/boilerplate/handout.py\", 25, 1)\n\ntests\\test_detector.py:158: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ...2])-1] + shapes[int(item[2])-1])\\n  m = max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests/sample_py/boilerplate/handout.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_ignore_leaf",
      "lineno": 172,
      "outcome": "failed",
      "keywords": [
        "test_ignore_leaf",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 176,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestParameters object at 0x0000020ECF7C3DA0>\n\n    def test_ignore_leaf(self):\n        detector = CopyDetector(test_dirs=[TESTS_DIR + \"/sample_py\"],\n                                ignore_leaf=True, silent=True)\n>       detector.run()\n\ntests\\test_detector.py:176: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_same_name_only",
      "lineno": 181,
      "outcome": "failed",
      "keywords": [
        "test_same_name_only",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 185,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestParameters object at 0x0000020ECF7C3F20>\n\n    def test_same_name_only(self):\n        detector = CopyDetector(test_dirs=[TESTS_DIR + \"/sample_py\"],\n                                same_name_only=True, silent=True)\n>       detector.run()\n\ntests\\test_detector.py:185: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_disable_filtering",
      "lineno": 189,
      "outcome": "passed",
      "keywords": [
        "test_disable_filtering",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_force_language",
      "lineno": 198,
      "outcome": "failed",
      "keywords": [
        "test_force_language",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 202,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestParameters object at 0x0000020ECF7FC170>\n\n    def test_force_language(self):\n        detector = CopyDetector(test_dirs=[TESTS_DIR + \"/sample_py\"],\n                                force_language=\"java\", silent=True)\n>       detector.run()\n\ntests\\test_detector.py:202: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = 'java'\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_truncation",
      "lineno": 209,
      "outcome": "failed",
      "keywords": [
        "test_truncation",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 215,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestParameters object at 0x0000020ECF7FC290>\n\n    def test_truncation(self):\n        detector = CopyDetector(\n            test_dirs=[TESTS_DIR + \"/sample_py/boilerplate\"],\n            noise_t=10, guarantee_t=10, truncate=True, silent=True)\n        detector.add_file(str(Path(TESTS_DIR + \"/sample_py/handout.py\")))\n>       detector.run()\n\ntests\\test_detector.py:215: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\n\\ndef generate_training_data_binary(num):\\n  if num ...2])-1] + shapes[int(item[2])-1])\\n  m = max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\boilerplate\\\\handout.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_out_file",
      "lineno": 219,
      "outcome": "failed",
      "keywords": [
        "test_out_file",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_detector.py",
            "lineno": 223,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 460,
            "message": "in run"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 380,
            "message": "in _preprocess_code"
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <test_detector.TestParameters object at 0x0000020ECF7FC4D0>\ntmpdir = local('C:\\\\Users\\\\Mohay\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Mohay\\\\pytest-1072\\\\test_out_file0')\n\n    def test_out_file(self, tmpdir):\n        detector = CopyDetector(test_dirs=[TESTS_DIR + \"/sample_py\"],\n            silent=True, out_file=tmpdir + \"/test\", autoopen=False)\n>       detector.run()\n\ntests\\test_detector.py:223: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:460: in run\n    self._preprocess_code(self.test_files + self.ref_files)\ncopydetect\\detector.py:380: in _preprocess_code\n    self.file_data[code_f] = CodeFingerprint(\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = 'import numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport svm\\n\\ndef generate_training_data_binary(num)...max(data.max(), abs(data.min()))+1\\n  plt.axis([-m, m, -m, m])\\n  plt.show()\\n\\nplot = plot_training_data_binary(data)'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests\\\\sample_py\\\\code\\\\sample2.py'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_detector.py::TestParameters::test_encoding_specification",
      "lineno": 238,
      "outcome": "passed",
      "keywords": [
        "test_encoding_specification",
        "TestParameters",
        "test_detector.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_empty",
      "lineno": 18,
      "outcome": "passed",
      "keywords": [
        "test_winnow_empty",
        "TestWinnowOutput",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_1",
      "lineno": 23,
      "outcome": "passed",
      "keywords": [
        "test_winnow_1",
        "TestWinnowOutput",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_2",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_winnow_2",
        "TestWinnowOutput",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_3",
      "lineno": 33,
      "outcome": "passed",
      "keywords": [
        "test_winnow_3",
        "TestWinnowOutput",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowOutput::test_winnow_inf",
      "lineno": 38,
      "outcome": "passed",
      "keywords": [
        "test_winnow_inf",
        "TestWinnowOutput",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_pywinnowing.py::TestWinnowDensity::test_winnow_density",
      "lineno": 47,
      "outcome": "passed",
      "keywords": [
        "test_winnow_density",
        "TestWinnowDensity",
        "test_pywinnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_sanity_checks.py::test_rot_c",
      "lineno": 12,
      "outcome": "failed",
      "keywords": [
        "test_rot_c",
        "test_sanity_checks.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_sanity_checks.py",
            "lineno": 17,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "def test_rot_c():\n        \"\"\"Make sure clearly copied c code is correctly reported as high\n        similarity\n        \"\"\"\n>       fp1 = CodeFingerprint(TESTS_DIR + \"/sample_sanity_check/rot1.c\", 25, 1)\n\ntests\\test_sanity_checks.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = '#include <stdio.h>\\n\\nint* rot_left(int* a, int len_a, int rot_count) {\\n    int curr_val = a[0];\\n    int next_val =... 2);\\n    for (int i = 0; i < 6; i++) {\\n        printf(\"%d \", arr[i]);\\n    }\\n    printf(\"\\\\n\");\\n\\n    return 0;\\n}'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests/sample_sanity_check/rot1.c'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_sanity_checks.py::test_sample_xml",
      "lineno": 23,
      "outcome": "failed",
      "keywords": [
        "test_sample_xml",
        "test_sanity_checks.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\copydetect\\utils.py",
          "lineno": 50,
          "message": "AttributeError: 'Text' object has no attribute 'isidentifier'"
        },
        "traceback": [
          {
            "path": "tests\\test_sanity_checks.py",
            "lineno": 28,
            "message": ""
          },
          {
            "path": "copydetect\\detector.py",
            "lineno": 111,
            "message": "in __init__"
          },
          {
            "path": "copydetect\\utils.py",
            "lineno": 50,
            "message": "AttributeError"
          }
        ],
        "longrepr": "def test_sample_xml():\n        \"\"\"Make sure identical XML file compared to itself returns 100%\n        similarity\n        \"\"\"\n>       fp1 = CodeFingerprint(TESTS_DIR + \"/sample_sanity_check/sample.xml\", 25, 1)\n\ntests\\test_sanity_checks.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ncopydetect\\detector.py:111: in __init__\n    filtered_code, offsets = filter_code(code, file, language)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncode = '<?xml version=\"1.0\"?>\\n<list>\\n   <item id=\"item1\">\\n      <field1>aa</field1>\\n      <field2>ab</field2>\\n      <fie...item id=\"item7\">\\n      <field1>ga</field1>\\n      <field2>gb</field2>\\n      <field3>gc</field3>\\n   </item>\\n</list>'\nfilename = 'D:\\\\repos\\\\blingenf@copydetect__ba072818__pygments__rich\\\\tests/sample_sanity_check/sample.xml'\nlanguage = None\n\n    def filter_code(code, filename, language=None):\n        \"\"\"Tokenize and filter a code document. Replace variable names with\n        V, function names with F, object names with O, and strings with S.\n        Return the filtered document and a list of offsets indicating how\n        many characters were removed by filtering at each index in the\n        resulting document where filtering occured (this is used later to\n        highlight the original code using plagiarism detection results on\n        the filtered code)\n        \"\"\"\n        try:\n            if language is not None:\n                syntax = Syntax(code, language, theme=\"monokai\", line_numbers=False)\n            else:\n                # Infer language from filename\n                syntax = Syntax.from_path(filename, theme=\"monokai\", line_numbers=False)\n        except Exception as e:\n            logging.warning(f\"{filename} not tokenized: {e}\")\n            return code, np.array([])\n    \n        # Extract tokens from the highlighted code\n        highlighted_code = syntax.highlight(code)\n        tokens = highlighted_code.split()  # Split into tokens (approximation)\n    \n        out_code = \"\"\n        offset = 0\n        offsets = [[0, 0]]\n        for token in tokens:\n            # Approximate token classification based on content\n>           if token.isidentifier():  # Variable or function names\nE           AttributeError: 'Text' object has no attribute 'isidentifier'\n\ncopydetect\\utils.py:50: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestSmallDoc::test_doc_overlap",
      "lineno": 18,
      "outcome": "passed",
      "keywords": [
        "test_doc_overlap",
        "TestSmallDoc",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestSmallDoc::test_doc_overlap_boilerplate",
      "lineno": 26,
      "outcome": "passed",
      "keywords": [
        "test_doc_overlap_boilerplate",
        "TestSmallDoc",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestSmallDoc::test_slice_computation",
      "lineno": 36,
      "outcome": "passed",
      "keywords": [
        "test_slice_computation",
        "TestSmallDoc",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestSmallDoc::test_highlighting",
      "lineno": 47,
      "outcome": "passed",
      "keywords": [
        "test_highlighting",
        "TestSmallDoc",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerPythonSample::test_tokenization",
      "lineno": 81,
      "outcome": "failed",
      "keywords": [
        "test_tokenization",
        "TestTokenizerPythonSample",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\tests\\test_utils.py",
          "lineno": 94,
          "message": "assert 'def hashed_k...array(hashes)' == 'defF(V,V):SV...]returnV.V(V)'\n  \n  - defF(V,V):SV=[hash(V[V:V+V])forVinrange(len(V)-V+1)]returnV.V(V)\n  + def hashed_kgrams(string, k):\n  +     \"\"\"Return hashes of all k-grams in string\"\"\"\n  +     hashes = [hash(string[offset:offset+k])\n  +               for offset in range(len(string) - k + 1)]\n  +     return np.array(hashes)"
        },
        "traceback": [
          {
            "path": "tests\\test_utils.py",
            "lineno": 94,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <test_utils.TestTokenizerPythonSample object at 0x0000020ECF7FE360>\n\n    def test_tokenization(self):\n        expected_out = (\"defF(V,V):SV=[hash(V[V:V+V])forV\"\n                        \"inrange(len(V)-V+1)]returnV.V(V)\")\n        expected_offsets = np.array([[0,0],[2,0],[3,1],[5,13],[6,18],[7,19],\n            [9,19],[9,20],[10,24],[10,67],[10,68],[11,72],[11,77],[12,78],\n            [19,79],[21,84],[23,89],[25,94],[27,94],[27,95],[30,109],[31,110],\n            [31,115],[33,116],[44,117],[45,122],[46,123],[47,124],[47,124],\n            [48,125],[51,126],[51,127],[57,131],[58,132],[60,133],[62,137],\n            [63,142]])\n    \n        out_code, offsets = cd.filter_code(self.sample_code, \"test.py\")\n    \n>       assert out_code == expected_out\nE       assert 'def hashed_k...array(hashes)' == 'defF(V,V):SV...]returnV.V(V)'\nE         \nE         - defF(V,V):SV=[hash(V[V:V+V])forVinrange(len(V)-V+1)]returnV.V(V)\nE         + def hashed_kgrams(string, k):\nE         +     \"\"\"Return hashes of all k-grams in string\"\"\"\nE         +     hashes = [hash(string[offset:offset+k])\nE         +               for offset in range(len(string) - k + 1)]\nE         +     return np.array(hashes)\n\ntests\\test_utils.py:94: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerPythonSample::test_copydetect",
      "lineno": 96,
      "outcome": "failed",
      "keywords": [
        "test_copydetect",
        "TestTokenizerPythonSample",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\tests\\test_utils.py",
          "lineno": 111,
          "message": "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed"
        },
        "traceback": [
          {
            "path": "tests\\test_utils.py",
            "lineno": 111,
            "message": "IndexError"
          }
        ],
        "longrepr": "self = <test_utils.TestTokenizerPythonSample object at 0x0000020ECF7FE510>\n\n    def test_copydetect(self):\n        out_code1, offsets1 = cd.filter_code(self.sample_code, \"1.py\")\n        out_code2, offsets2 = cd.filter_code(self.sample_copied_code, \"2.py\")\n    \n        hashes1, idx1 = cd.get_document_fingerprints(out_code1, 20, 1)\n        hashes2, idx2 = cd.get_document_fingerprints(out_code2, 20, 1)\n        ol_idx1, ol_idx2 = cd.find_fingerprint_overlap(hashes1, hashes2,\n                                                       idx1, idx2)\n        slices1 = cd.get_copied_slices(ol_idx1, 20)\n        slices2 = cd.get_copied_slices(ol_idx2, 20)\n    \n        hl_filter1, _ = cd.highlight_overlap(out_code1, slices1, \">\", \"<\")\n        hl_filter2, _ = cd.highlight_overlap(out_code2, slices2, \">\", \"<\")\n    \n>       slices1 += offsets1[:,1][np.clip(\n            np.searchsorted(offsets1[:,0], slices1), 0, offsets1.shape[0] - 1)]\nE       IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\ntests\\test_utils.py:111: IndexError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_php_tokenization",
      "lineno": 139,
      "outcome": "failed",
      "keywords": [
        "test_php_tokenization",
        "TestTokenizerOtherSamples",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\tests\\test_utils.py",
          "lineno": 147,
          "message": "AssertionError: assert 'var1' not in '<!DOCTYPE H...>\\n</html>\\n'\n  \n  'var1' is contained here:\n    /a>';\n    \n      $var1 = 1;\n  ?    ++++\n      $var5 = $var1 + 4;...\n  \n  ...Full output truncated (3 lines hidden), use '-vv' to show"
        },
        "traceback": [
          {
            "path": "tests\\test_utils.py",
            "lineno": 147,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <test_utils.TestTokenizerOtherSamples object at 0x0000020ECF7FE6F0>\n\n    def test_php_tokenization(self):\n        with open(TESTS_DIR + \"/sample_other/php_sample.php\") as php_f:\n            php_sample = php_f.read()\n        out_code, offsets = cd.filter_code(php_sample, \"php_sample.php\")\n    \n        # PHP variables are tokenized differently because they\n        # have a $ prefix\n>       assert \"var1\" not in out_code\nE       AssertionError: assert 'var1' not in '<!DOCTYPE H...>\\n</html>\\n'\nE         \nE         'var1' is contained here:\nE           /a>';\nE           \nE             $var1 = 1;\nE         ?    ++++\nE             $var5 = $var1 + 4;...\nE         \nE         ...Full output truncated (3 lines hidden), use '-vv' to show\n\ntests\\test_utils.py:147: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_c_tokenization",
      "lineno": 151,
      "outcome": "failed",
      "keywords": [
        "test_c_tokenization",
        "TestTokenizerOtherSamples",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\tests\\test_utils.py",
          "lineno": 159,
          "message": "assert 'PintF' in '#include <stdio.h>\\n\\nint add_num(int a, int b) {\\n    return a + b;\\n}\\n\\nint main(int argc, char* argv[]) {\\n    int a = 2;\\n    int b = 2;\\n    int result = add_num(a, b);\\n    printf(\"%d + %d = 5\\\\n\", a, b);\\n\\n    return 0;\\n}\\n'"
        },
        "traceback": [
          {
            "path": "tests\\test_utils.py",
            "lineno": 159,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <test_utils.TestTokenizerOtherSamples object at 0x0000020ECF7FE870>\n\n    def test_c_tokenization(self):\n        with open(TESTS_DIR + \"/sample_other/c_sample.c\") as c_f:\n            c_sample = c_f.read()\n        out_code, offsets = cd.filter_code(c_sample, \"c_sample.c\")\n    \n        # Preprocessor directive and function name should be filtered,\n        # return type should not\n>       assert \"PintF\" in out_code\nE       assert 'PintF' in '#include <stdio.h>\\n\\nint add_num(int a, int b) {\\n    return a + b;\\n}\\n\\nint main(int argc, char* argv[]) {\\n    int a = 2;\\n    int b = 2;\\n    int result = add_num(a, b);\\n    printf(\"%d + %d = 5\\\\n\", a, b);\\n\\n    return 0;\\n}\\n'\n\ntests\\test_utils.py:159: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_java_tokenization",
      "lineno": 162,
      "outcome": "failed",
      "keywords": [
        "test_java_tokenization",
        "TestTokenizerOtherSamples",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\blingenf@copydetect__ba072818__pygments__rich\\tests\\test_utils.py",
          "lineno": 169,
          "message": "assert 'this.V=V' in 'import java.util.Random;\\nimport java.util.regex.Matcher;\\nimport java.util.regex.Pattern;\\nimport java.lang.IllegalA...+ sides + \" + \" + modifier;\\n        else\\n            return count + \"d\" + sides + \" - \" + modifier * -1;\\n    }\\n}\\n'"
        },
        "traceback": [
          {
            "path": "tests\\test_utils.py",
            "lineno": 169,
            "message": "AssertionError"
          }
        ],
        "longrepr": "self = <test_utils.TestTokenizerOtherSamples object at 0x0000020ECF7FE9F0>\n\n    def test_java_tokenization(self):\n        with open(TESTS_DIR + \"/sample_other/java_sample.java\") as java_f:\n            java_sample = java_f.read()\n        out_code, offsets = cd.filter_code(java_sample, \"java_sample.java\")\n    \n        # member variables should be tokenized\n>       assert \"this.V=V\" in out_code\nE       assert 'this.V=V' in 'import java.util.Random;\\nimport java.util.regex.Matcher;\\nimport java.util.regex.Pattern;\\nimport java.lang.IllegalA...+ sides + \" + \" + modifier;\\n        else\\n            return count + \"d\" + sides + \" - \" + modifier * -1;\\n    }\\n}\\n'\n\ntests\\test_utils.py:169: AssertionError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_utils.py::TestTokenizerOtherSamples::test_get_token_coverage",
      "lineno": 175,
      "outcome": "passed",
      "keywords": [
        "test_get_token_coverage",
        "TestTokenizerOtherSamples",
        "test_utils.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_empty",
      "lineno": 28,
      "outcome": "passed",
      "keywords": [
        "test_winnow_empty",
        "TestWinnowOutput",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_1",
      "lineno": 33,
      "outcome": "passed",
      "keywords": [
        "test_winnow_1",
        "TestWinnowOutput",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_2",
      "lineno": 38,
      "outcome": "passed",
      "keywords": [
        "test_winnow_2",
        "TestWinnowOutput",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_3",
      "lineno": 43,
      "outcome": "passed",
      "keywords": [
        "test_winnow_3",
        "TestWinnowOutput",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowOutput::test_winnow_inf",
      "lineno": 48,
      "outcome": "passed",
      "keywords": [
        "test_winnow_inf",
        "TestWinnowOutput",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/test_winnowing.py::TestWinnowDensity::test_winnow_density",
      "lineno": 57,
      "outcome": "passed",
      "keywords": [
        "test_winnow_density",
        "TestWinnowDensity",
        "test_winnowing.py",
        "tests",
        "blingenf@copydetect__ba072818__pygments__rich",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    }
  ]
}