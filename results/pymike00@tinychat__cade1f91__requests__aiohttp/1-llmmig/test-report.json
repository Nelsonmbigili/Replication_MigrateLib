{
  "exitcode": 1,
  "summary": {
    "passed": 5,
    "failed": 16,
    "total": 21,
    "collected": 21
  },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": ".",
          "type": "Dir"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 50
        },
        {
          "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 10
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_anthropic_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_anthropic_handler.py::TestAnthropicAIHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_anthropic_handler.py::TestAnthropicAIHandlerStreaming::test_stream_response_success",
          "type": "TestCaseFunction",
          "lineno": 9
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_anthropic_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_anthropic_handler.py::TestAnthropicAIHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 28
        },
        {
          "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 9
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_cohere_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_cohere_handler.py::TestCohereHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_cohere_handler.py::TestCohereHandlerStreaming::test_stream_response_success",
          "type": "TestCaseFunction",
          "lineno": 7
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_cohere_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_cohere_handler.py::TestCohereHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 53
        },
        {
          "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 10
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_google_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_google_handler.py::TestGoogleGeminiHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_google_handler.py::TestGoogleGeminiHandlerStreaming::test_stream_response",
          "type": "TestCaseFunction",
          "lineno": 8
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_google_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_google_handler.py::TestGoogleGeminiHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_api_key_setter_invalid_key",
          "type": "TestCaseFunction",
          "lineno": 15
        },
        {
          "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_api_key_setter_valid_key",
          "type": "TestCaseFunction",
          "lineno": 9
        },
        {
          "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_default_headers",
          "type": "TestCaseFunction",
          "lineno": 23
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_llms_base.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 50
        },
        {
          "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 11
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_mistral_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_mistral_handler.py::TestMistralHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_mistral_handler.py::TestMistralHandlerStreaming::test_stream_response",
          "type": "TestCaseFunction",
          "lineno": 8
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_mistral_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_mistral_handler.py::TestMistralHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 53
        },
        {
          "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 11
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_openai_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_openai_handler.py::TestOpenAIHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_openai_handler.py::TestOpenAIHandlerStreaming::test_stream_response",
          "type": "TestCaseFunction",
          "lineno": 8
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_openai_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_openai_handler.py::TestOpenAIHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming::test_perform_stream_request_failure",
          "type": "TestCaseFunction",
          "lineno": 53
        },
        {
          "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming::test_perform_stream_request_success",
          "type": "TestCaseFunction",
          "lineno": 11
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_together_client.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_together_handler.py::TestTogetherHandlerStreaming",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_together_handler.py::TestTogetherHandlerStreaming::test_stream_response",
          "type": "TestCaseFunction",
          "lineno": 8
        }
      ]
    },
    {
      "nodeid": "tests/llms/test_together_handler.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_together_handler.py::TestTogetherHandlerStreaming",
          "type": "UnitTestCase"
        }
      ]
    },
    {
      "nodeid": "tests/llms",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms/test_anthropic_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_anthropic_handler.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_cohere_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_cohere_handler.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_google_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_google_handler.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_llms_base.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_mistral_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_mistral_handler.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_openai_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_openai_handler.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_together_client.py",
          "type": "Module"
        },
        {
          "nodeid": "tests/llms/test_together_handler.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests/llms",
          "type": "Package"
        }
      ]
    },
    {
      "nodeid": "tinychat/backend",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tinychat/llms",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tinychat/ui",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tinychat/utils",
      "outcome": "passed",
      "result": []
    },
    {
      "nodeid": "tinychat",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tinychat/backend",
          "type": "Package"
        },
        {
          "nodeid": "tinychat/llms",
          "type": "Package"
        },
        {
          "nodeid": "tinychat/ui",
          "type": "Package"
        },
        {
          "nodeid": "tinychat/utils",
          "type": "Package"
        }
      ]
    },
    {
      "nodeid": ".",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests",
          "type": "Package"
        },
        {
          "nodeid": "tinychat",
          "type": "Package"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming::test_perform_stream_request_failure",
      "lineno": 50,
      "outcome": "passed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestAnthropicAIClientStreaming",
        "test_anthropic_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_anthropic_client.py::TestAnthropicAIClientStreaming::test_perform_stream_request_success",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestAnthropicAIClientStreaming",
        "test_anthropic_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
          "lineno": 1431,
          "message": "AttributeError: <module 'tinychat.llms.anthropic' from 'D:\\\\repos\\\\pymike00@tinychat__cade1f91__requests__aiohttp\\\\tinychat\\\\llms\\\\anthropic.py'> does not have the attribute 'SSEClient'"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_anthropic_client.py",
            "lineno": 34,
            "message": ""
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1458,
            "message": "in __enter__"
          },
          {
            "path": "C:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py",
            "lineno": 1431,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_anthropic_client.TestAnthropicAIClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751948470864'>\nmock_post = <MagicMock name='post' id='1751948474704'>\n\n    @patch(\"tinychat.llms.anthropic.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    async def test_perform_stream_request_success(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking the response\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 200\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        # Creating mock events\n        mock_event1 = MagicMock()\n        mock_event1.data = json.dumps(\n            {\"type\": \"content_block_delta\", \"delta\": {\"text\": \"part1\"}}\n        )\n        mock_event2 = MagicMock()\n        mock_event2.data = json.dumps(\n            {\"type\": \"content_block_delta\", \"delta\": {\"text\": \"part2\"}}\n        )\n        mock_event_done = MagicMock()\n        mock_event_done.data = json.dumps({\"type\": \"done\"})\n    \n        # Mocking SSEClient to return test events\n>       with patch(\"tinychat.llms.anthropic.SSEClient\") as mock_sse_client:\n\ntests\\llms\\test_anthropic_client.py:34: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1458: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000197E8373C20>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'tinychat.llms.anthropic' from 'D:\\\\repos\\\\pymike00@tinychat__cade1f91__requests__aiohttp\\\\tinychat\\\\llms\\\\anthropic.py'> does not have the attribute 'SSEClient'\n\nC:\\Users\\Mohay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\unittest\\mock.py:1431: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_anthropic_handler.py::TestAnthropicAIHandlerStreaming::test_stream_response_success",
      "lineno": 9,
      "outcome": "failed",
      "keywords": [
        "test_stream_response_success",
        "__wrapped__",
        "patchings",
        "TestAnthropicAIHandlerStreaming",
        "test_anthropic_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_anthropic_handler.py",
          "lineno": 58,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_anthropic_handler.py",
            "lineno": 58,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_anthropic_handler.TestAnthropicAIHandlerStreaming testMethod=test_stream_response_success>\nmock_api_key = <MagicMock name='api_key' id='1751947619232'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751947622016'>\n\n    @patch.object(AnthropicAIClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response_success(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Create a mock SSEClient with a mock events method\n        mock_sse_client = MagicMock()\n        mock_stream = iter(\n            [\n                Mock(\n                    data=json.dumps(\n                        {\n                            \"type\": \"content_block_start\",\n                            \"index\": 0,\n                            \"content_block\": {\"type\": \"text\", \"text\": \"\"},\n                        }\n                    )\n                ),\n                Mock(data=json.dumps({\"type\": \"ping\"})),\n                Mock(\n                    data=json.dumps(\n                        {\n                            \"type\": \"content_block_delta\",\n                            \"index\": 0,\n                            \"delta\": {\"type\": \"text_delta\", \"text\": \"Hello\"},\n                        }\n                    )\n                ),\n                Mock(\n                    data=json.dumps(\n                        {\n                            \"type\": \"content_block_delta\",\n                            \"index\": 0,\n                            \"delta\": {\"type\": \"text_delta\", \"text\": \"!\"},\n                        }\n                    )\n                ),\n                Mock(data=json.dumps({\"type\": \"content_block_stop\", \"index\": 0})),\n            ]\n        )\n        mock_sse_client.events.return_value = mock_stream\n        mock_perform_stream_request.return_value = mock_sse_client\n    \n        handler = AnthropicAIHandler(model_name=\"test_model\")\n        generator = handler.stream_response(\"hello\")\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for part in generator:\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_anthropic_handler.py:58: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming::test_perform_stream_request_failure",
      "lineno": 28,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestCohereClientStreaming",
        "test_cohere_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_cohere_client.py",
          "lineno": 45,
          "message": "TypeError: object async_generator can't be used in 'await' expression"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_cohere_client.py",
            "lineno": 45,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_cohere_client.TestCohereClientStreaming testMethod=test_perform_stream_request_failure>\nmock_api_key = <MagicMock name='api_key' id='1751947478224'>\nmock_post = <MagicMock name='post' id='1751947483264'>\n\n    @patch(\"tinychat.llms.cohere.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    async def test_perform_stream_request_failure(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking aiohttp response\n        mock_response = AsyncMock()\n        mock_response.status = 400\n        mock_post.return_value = mock_response\n    \n        client = CohereClient(temperature=0.0)\n        chat_history = [\n            {\"role\": \"User\", \"message\": \"hello\"},\n            {\"role\": \"Chatbot\", \"message\": \"hello!\"},\n        ]\n        with self.assertRaises(ValueError) as context:\n>           await client.perform_stream_request(\"how are you?\", chat_history)\nE           TypeError: object async_generator can't be used in 'await' expression\n\ntests\\llms\\test_cohere_client.py:45: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_cohere_client.py::TestCohereClientStreaming::test_perform_stream_request_success",
      "lineno": 9,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestCohereClientStreaming",
        "test_cohere_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_cohere_client.py",
          "lineno": 25,
          "message": "TypeError: object async_generator can't be used in 'await' expression"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_cohere_client.py",
            "lineno": 25,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_cohere_client.TestCohereClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751949526512'>\nmock_post = <MagicMock name='post' id='1751949528960'>\n\n    @patch(\"tinychat.llms.cohere.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    async def test_perform_stream_request_success(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking aiohttp response\n        mock_response = AsyncMock()\n        mock_response.status = 200\n        mock_post.return_value = mock_response\n    \n        client = CohereClient(temperature=0.0)\n        chat_history = [\n            {\"role\": \"User\", \"message\": \"hello\"},\n            {\"role\": \"Chatbot\", \"message\": \"hello!\"},\n        ]\n>       response = await client.perform_stream_request(\"how are you?\", chat_history)\nE       TypeError: object async_generator can't be used in 'await' expression\n\ntests\\llms\\test_cohere_client.py:25: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_cohere_handler.py::TestCohereHandlerStreaming::test_stream_response_success",
      "lineno": 7,
      "outcome": "failed",
      "keywords": [
        "test_stream_response_success",
        "__wrapped__",
        "patchings",
        "TestCohereHandlerStreaming",
        "test_cohere_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_cohere_handler.py",
          "lineno": 23,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_cohere_handler.py",
            "lineno": 23,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_cohere_handler.TestCohereHandlerStreaming testMethod=test_stream_response_success>\nmock_api_key = <MagicMock name='api_key' id='1751949537456'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751950016976'>\n\n    @patch.object(CohereClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response_success(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking the stream of responses\n        mock_stream = [b'{\"event_type\": \"text-generation\", \"text\": \"Hi!\"}',\n                       b'{\"event_type\": \"text-generation\", \"text\": \"How can I help?\"}']\n        mock_perform_stream_request.return_value = mock_stream\n    \n        handler = CohereHandler()\n        user_input = \"hello\"\n        generator = handler.stream_response(user_input)\n    \n        # Collecting responses from the generator\n>       responses = [resp for resp in generator]\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_cohere_handler.py:23: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming::test_perform_stream_request_failure",
      "lineno": 53,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestGoogleAIClientStreaming",
        "test_google_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_google_client.py",
          "lineno": 68,
          "message": "TypeError: 'async_generator' object is not an iterator"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_google_client.py",
            "lineno": 68,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_google_client.TestGoogleAIClientStreaming testMethod=test_perform_stream_request_failure>\nmock_api_key = <MagicMock name='api_key' id='1751950022496'>\nmock_post = <MagicMock name='post' id='1751950026480'>\n\n    @patch(\"tinychat.llms.google.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_failure(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking the response with an error status code\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 400\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        client = GoogleAIClient(temperature=0.0)\n        messages = [{\"parts\": [{\"text\": \"content\"}], \"role\": \"user\"}]\n    \n        with self.assertRaises(ValueError) as context:\n>           next(client.perform_stream_request(messages))  # type: ignore\nE           TypeError: 'async_generator' object is not an iterator\n\ntests\\llms\\test_google_client.py:68: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_google_client.py::TestGoogleAIClientStreaming::test_perform_stream_request_success",
      "lineno": 10,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestGoogleAIClientStreaming",
        "test_google_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_google_client.py",
          "lineno": 44,
          "message": "AttributeError: 'async_generator' object has no attribute 'events'"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_google_client.py",
            "lineno": 44,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_google_client.TestGoogleAIClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751949538848'>\nmock_sse_client = <MagicMock name='SSEClient' id='1751950002224'>\nmock_post = <MagicMock name='post' id='1751950006592'>\n\n    @patch(\"tinychat.llms.google.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.google.SSEClient\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_success(self, mock_api_key, mock_sse_client, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking SSEClient and the response\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 200\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        # Creating mock events\n        mock_event1 = MagicMock()\n        mock_event1.data = json.dumps(\n            {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"part1\"}]}}]}\n        )\n        mock_event2 = MagicMock()\n        mock_event2.data = json.dumps(\n            {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"part2\"}]}}]}\n        )\n        mock_event_done = MagicMock()\n        mock_event_done.data = \"[DONE]\"\n    \n        # Setting up a test stream\n        test_stream = [mock_event1, mock_event2, mock_event_done]\n        mock_sse_client.return_value.events = MagicMock(return_value=iter(test_stream))\n    \n        client = GoogleAIClient(temperature=0.0)\n        messages = [{\"parts\": [{\"text\": \"content\"}], \"role\": \"user\"}]\n        stream = client.perform_stream_request(messages)\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for event in stream.events():\nE       AttributeError: 'async_generator' object has no attribute 'events'\n\ntests\\llms\\test_google_client.py:44: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_google_handler.py::TestGoogleGeminiHandlerStreaming::test_stream_response",
      "lineno": 8,
      "outcome": "failed",
      "keywords": [
        "test_stream_response",
        "__wrapped__",
        "patchings",
        "TestGoogleGeminiHandlerStreaming",
        "test_google_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_google_handler.py",
          "lineno": 46,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_google_handler.py",
            "lineno": 46,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_google_handler.TestGoogleGeminiHandlerStreaming testMethod=test_stream_response>\nmock_api_key = <MagicMock name='api_key' id='1751949907760'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751949912032'>\n\n    @patch.object(GoogleAIClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Create a mock SSEClient with a mock events method\n        mock_sse_client = MagicMock()\n        mock_stream = iter(\n            [\n                Mock(\n                    data=json.dumps(\n                        {\n                            \"candidates\": [\n                                {\"content\": {\"parts\": [{\"text\": \"response part 1\"}]}}\n                            ]\n                        }\n                    )\n                ),\n                Mock(\n                    data=json.dumps(\n                        {\n                            \"candidates\": [\n                                {\"content\": {\"parts\": [{\"text\": \"response part 2\"}]}}\n                            ]\n                        }\n                    )\n                ),\n            ]\n        )\n        mock_sse_client.events.return_value = mock_stream\n        mock_perform_stream_request.return_value = mock_sse_client\n    \n        handler = GoogleAIHandler()\n        generator = handler.stream_response(\"hello\")\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for part in generator:\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_google_handler.py:46: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_api_key_setter_invalid_key",
      "lineno": 15,
      "outcome": "passed",
      "keywords": [
        "test_api_key_setter_invalid_key",
        "__wrapped__",
        "patchings",
        "TestBaseLLMClient",
        "test_llms_base.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_api_key_setter_valid_key",
      "lineno": 9,
      "outcome": "passed",
      "keywords": [
        "test_api_key_setter_valid_key",
        "__wrapped__",
        "patchings",
        "TestBaseLLMClient",
        "test_llms_base.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_llms_base.py::TestBaseLLMClient::test_default_headers",
      "lineno": 23,
      "outcome": "passed",
      "keywords": [
        "test_default_headers",
        "__wrapped__",
        "patchings",
        "TestBaseLLMClient",
        "test_llms_base.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming::test_perform_stream_request_failure",
      "lineno": 50,
      "outcome": "passed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestMistralClientStreaming",
        "test_mistral_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "passed"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_mistral_client.py::TestMistralClientStreaming::test_perform_stream_request_success",
      "lineno": 11,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestMistralClientStreaming",
        "test_mistral_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_mistral_client.py",
          "lineno": 38,
          "message": "TypeError: object async_generator can't be used in 'await' expression"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_mistral_client.py",
            "lineno": 38,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_mistral_client.TestMistralClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751949769136'>\nmock_sse_client = <MagicMock name='SSEClient' id='1751949609008'>\nmock_post = <MagicMock name='post' id='1751949612800'>\n\n    @patch(\"tinychat.llms.mistral.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.mistral.SSEClient\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    async def test_perform_stream_request_success(self, mock_api_key, mock_sse_client, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Setup: Mocking SSEClient and the response\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 200\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        # Creating mock events\n        mock_event1 = MagicMock()\n        mock_event1.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part1\"}}]})\n        mock_event2 = MagicMock()\n        mock_event2.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part2\"}}]})\n        mock_event_done = MagicMock()\n        mock_event_done.data = \"[DONE]\"\n    \n        # Setting up a test stream\n        test_stream = [mock_event1, mock_event2, mock_event_done]\n        mock_sse_client.return_value.events = MagicMock(return_value=iter(test_stream))\n    \n        # Execution\n        client = MistralClient(model_name=\"test_model\", temperature=0.0)\n        messages = [{\"role\": \"user\", \"content\": \"hello\"}]\n>       stream = await client.perform_stream_request(messages)\nE       TypeError: object async_generator can't be used in 'await' expression\n\ntests\\llms\\test_mistral_client.py:38: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_mistral_handler.py::TestMistralHandlerStreaming::test_stream_response",
      "lineno": 8,
      "outcome": "failed",
      "keywords": [
        "test_stream_response",
        "__wrapped__",
        "patchings",
        "TestMistralHandlerStreaming",
        "test_mistral_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_mistral_handler.py",
          "lineno": 39,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_mistral_handler.py",
            "lineno": 39,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_mistral_handler.TestMistralHandlerStreaming testMethod=test_stream_response>\nmock_api_key = <MagicMock name='api_key' id='1751949757472'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751949759440'>\n\n    @patch.object(MistralClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Create a mock SSEClient with a mock events method\n        mock_sse_client = MagicMock()\n        mock_stream = iter(\n            [\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 1\"}}]}\n                    )\n                ),\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 2\"}}]}\n                    )\n                ),\n                Mock(data=\"[DONE]\"),\n            ]\n        )\n        mock_sse_client.events.return_value = mock_stream\n        mock_perform_stream_request.return_value = mock_sse_client\n    \n        handler = MistralHandler(model_name=\"test_model\")\n        generator = handler.stream_response(\"hello\")\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for part in generator:\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_mistral_handler.py:39: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming::test_perform_stream_request_failure",
      "lineno": 53,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestOpenAIClientStreaming",
        "test_openai_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_openai_client.py",
          "lineno": 68,
          "message": "TypeError: 'async_generator' object is not an iterator"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_openai_client.py",
            "lineno": 68,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_openai_client.TestOpenAIClientStreaming testMethod=test_perform_stream_request_failure>\nmock_api_key = <MagicMock name='api_key' id='1751949870912'>\nmock_post = <MagicMock name='post' id='1751949916688'>\n\n    @patch(\"tinychat.llms.openai.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_failure(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking the response with an error status code\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 400\n        mock_post.return_value = mock_response\n    \n        client = OpenAIClient(model_name=\"test_model\", temperature=0.0)\n        messages = [{\"role\": \"user\", \"content\": \"hello\"}]\n    \n        with self.assertRaises(ValueError) as context:\n>           next(client.perform_stream_request(messages))  # type: ignore\nE           TypeError: 'async_generator' object is not an iterator\n\ntests\\llms\\test_openai_client.py:68: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_openai_client.py::TestOpenAIClientStreaming::test_perform_stream_request_success",
      "lineno": 11,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestOpenAIClientStreaming",
        "test_openai_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_openai_client.py",
          "lineno": 43,
          "message": "AttributeError: 'async_generator' object has no attribute 'events'"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_openai_client.py",
            "lineno": 43,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_openai_client.TestOpenAIClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751949910160'>\nmock_sse_client = <MagicMock name='SSEClient' id='1751949902144'>\nmock_post = <MagicMock name='post' id='1751949948304'>\n\n    @patch(\"tinychat.llms.openai.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.openai.SSEClient\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_success(\n        self, mock_api_key, mock_sse_client, mock_post\n    ):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking SSEClient and the response\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 200\n        mock_post.return_value = mock_response\n    \n        # Creating mock events\n        mock_event1 = MagicMock()\n        mock_event1.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part1\"}}]})\n        mock_event2 = MagicMock()\n        mock_event2.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part2\"}}]})\n        mock_event_done = MagicMock()\n        mock_event_done.data = \"[DONE]\"\n    \n        # Setting up a test stream\n        test_stream = [mock_event1, mock_event2, mock_event_done]\n        mock_sse_client.return_value.events = MagicMock(return_value=iter(test_stream))\n    \n        client = OpenAIClient(model_name=\"test_model\", temperature=0.0)\n        messages = [{\"role\": \"user\", \"content\": \"hello\"}]\n        stream = client.perform_stream_request(messages)\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for event in stream.events():\nE       AttributeError: 'async_generator' object has no attribute 'events'\n\ntests\\llms\\test_openai_client.py:43: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_openai_handler.py::TestOpenAIHandlerStreaming::test_stream_response",
      "lineno": 8,
      "outcome": "failed",
      "keywords": [
        "test_stream_response",
        "__wrapped__",
        "patchings",
        "TestOpenAIHandlerStreaming",
        "test_openai_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_openai_handler.py",
          "lineno": 39,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_openai_handler.py",
            "lineno": 39,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_openai_handler.TestOpenAIHandlerStreaming testMethod=test_stream_response>\nmock_api_key = <MagicMock name='api_key' id='1751950008944'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751950004432'>\n\n    @patch.object(OpenAIClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Create a mock SSEClient with a mock events method\n        mock_sse_client = MagicMock()\n        mock_stream = iter(\n            [\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 1\"}}]}\n                    )\n                ),\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 2\"}}]}\n                    )\n                ),\n                Mock(data=\"[DONE]\"),\n            ]\n        )\n        mock_sse_client.events.return_value = mock_stream\n        mock_perform_stream_request.return_value = mock_sse_client\n    \n        handler = OpenAIHandler(model_name=\"test_model\")\n        generator = handler.stream_response(\"hello\")\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for part in generator:\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_openai_handler.py:39: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming::test_perform_stream_request_failure",
      "lineno": 53,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_failure",
        "__wrapped__",
        "patchings",
        "TestTogetherClientStreaming",
        "test_together_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_together_client.py",
          "lineno": 68,
          "message": "TypeError: 'async_generator' object is not an iterator"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_together_client.py",
            "lineno": 68,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_together_client.TestTogetherClientStreaming testMethod=test_perform_stream_request_failure>\nmock_api_key = <MagicMock name='api_key' id='1751950029264'>\nmock_post = <MagicMock name='post' id='1751950024848'>\n\n    @patch(\"tinychat.llms.together.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_failure(self, mock_api_key, mock_post):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking the response with an error status code\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 400\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        client = TogetherClient(model_name=\"test_model\", temperature=0.0)\n        messages = [{\"role\": \"user\", \"content\": \"hello\"}]\n    \n        with self.assertRaises(ValueError) as context:\n>           next(client.perform_stream_request(messages))  # type: ignore\nE           TypeError: 'async_generator' object is not an iterator\n\ntests\\llms\\test_together_client.py:68: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_together_client.py::TestTogetherClientStreaming::test_perform_stream_request_success",
      "lineno": 11,
      "outcome": "failed",
      "keywords": [
        "test_perform_stream_request_success",
        "__wrapped__",
        "patchings",
        "TestTogetherClientStreaming",
        "test_together_client.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_together_client.py",
          "lineno": 43,
          "message": "AttributeError: 'async_generator' object has no attribute 'events'"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_together_client.py",
            "lineno": 43,
            "message": "AttributeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_together_client.TestTogetherClientStreaming testMethod=test_perform_stream_request_success>\nmock_api_key = <MagicMock name='api_key' id='1751949527856'>\nmock_sse_client = <MagicMock name='SSEClient' id='1751949162448'>\nmock_post = <MagicMock name='post' id='1751925828704'>\n\n    @patch(\"tinychat.llms.together.aiohttp.ClientSession.post\")\n    @patch(\"tinychat.llms.together.SSEClient\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_perform_stream_request_success(\n        self, mock_api_key, mock_sse_client, mock_post\n    ):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Mocking SSEClient and the response\n        mock_response = AsyncMock(spec=ClientResponse)\n        mock_response.status = 200\n        mock_post.return_value.__aenter__.return_value = mock_response\n    \n        # Creating mock events\n        mock_event1 = MagicMock()\n        mock_event1.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part1\"}}]})\n        mock_event2 = MagicMock()\n        mock_event2.data = json.dumps({\"choices\": [{\"delta\": {\"content\": \"part2\"}}]})\n        mock_event_done = MagicMock()\n        mock_event_done.data = \"[DONE]\"\n    \n        # Setting up a test stream\n        test_stream = [mock_event1, mock_event2, mock_event_done]\n        mock_sse_client.return_value.events = MagicMock(return_value=iter(test_stream))\n    \n        client = TogetherClient(model_name=\"test_model\", temperature=0.0)\n        messages = [{\"role\": \"user\", \"content\": \"hello\"}]\n        stream = client.perform_stream_request(messages)\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for event in stream.events():\nE       AttributeError: 'async_generator' object has no attribute 'events'\n\ntests\\llms\\test_together_client.py:43: AttributeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    },
    {
      "nodeid": "tests/llms/test_together_handler.py::TestTogetherHandlerStreaming::test_stream_response",
      "lineno": 8,
      "outcome": "failed",
      "keywords": [
        "test_stream_response",
        "__wrapped__",
        "patchings",
        "TestTogetherHandlerStreaming",
        "test_together_handler.py",
        "llms",
        "tests",
        "pymike00@tinychat__cade1f91__requests__aiohttp",
        ""
      ],
      "setup": {
        "outcome": "passed"
      },
      "call": {
        "outcome": "failed",
        "crash": {
          "path": "D:\\repos\\pymike00@tinychat__cade1f91__requests__aiohttp\\tests\\llms\\test_together_handler.py",
          "lineno": 39,
          "message": "TypeError: 'async_generator' object is not iterable"
        },
        "traceback": [
          {
            "path": "tests\\llms\\test_together_handler.py",
            "lineno": 39,
            "message": "TypeError"
          }
        ],
        "longrepr": "self = <tests.llms.test_together_handler.TestTogetherHandlerStreaming testMethod=test_stream_response>\nmock_api_key = <MagicMock name='api_key' id='1751949840400'>\nmock_perform_stream_request = <MagicMock name='perform_stream_request' id='1751949844288'>\n\n    @patch.object(TogetherClient, \"perform_stream_request\")\n    @patch(\"tinychat.llms.base.BaseLLMClient.api_key\", new_callable=MagicMock)\n    def test_stream_response(self, mock_api_key, mock_perform_stream_request):\n        # Setting a dummy value for mock_api_key is not strictly needed here\n    \n        # Create a mock SSEClient with a mock events method\n        mock_sse_client = MagicMock()\n        mock_stream = iter(\n            [\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 1\"}}]}\n                    )\n                ),\n                Mock(\n                    data=json.dumps(\n                        {\"choices\": [{\"delta\": {\"content\": \"response part 2\"}}]}\n                    )\n                ),\n                Mock(data=\"[DONE]\"),\n            ]\n        )\n        mock_sse_client.events.return_value = mock_stream\n        mock_perform_stream_request.return_value = mock_sse_client\n    \n        handler = TogetherHandler(model_name=\"test_model\")\n        generator = handler.stream_response(\"hello\")\n    \n        # Extracting and verifying the stream response\n        responses = []\n>       for part in generator:\nE       TypeError: 'async_generator' object is not iterable\n\ntests\\llms\\test_together_handler.py:39: TypeError"
      },
      "teardown": {
        "outcome": "passed"
      }
    }
  ]
}